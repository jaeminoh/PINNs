{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physics-informed deep operator network for 1d Poisson equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Poissson equation, PI-DeepONet\n",
    "    x in [0, 1]\n",
    "    u_xx = -a * pi^2 sin(pi x)\n",
    "    u(0) = u(1) = 0\n",
    "    u(x) = a * sin(pi x)\n",
    "\"\"\"\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "import optax\n",
    "\n",
    "from nn import MLP\n",
    "\n",
    "\n",
    "# define MLP\n",
    "def MLP(layers: list[int] = [1, 64, 1], activation: callable = jnp.tanh):\n",
    "    def init_params(key):\n",
    "        def _init(key, d_in, d_out):\n",
    "            w = jr.normal(key, shape=(d_in, d_out)) * jnp.sqrt(2 / (d_in + d_out))\n",
    "            b = jnp.zeros((d_out,))\n",
    "            return [w, b]\n",
    "\n",
    "        keys = jr.split(key, len(layers) - 1)\n",
    "        params = list(map(_init, keys, layers[:-1], layers[1:]))\n",
    "        return params\n",
    "\n",
    "    def apply(params, inputs):\n",
    "        for W, b in params[:-1]:\n",
    "            outputs = inputs @ W + b\n",
    "            inputs = activation(outputs)\n",
    "        W, b = params[-1]\n",
    "        outputs = inputs @ W + b\n",
    "        return outputs\n",
    "\n",
    "    return init_params, apply\n",
    "\n",
    "\n",
    "# forcing term.\n",
    "def f(x, a):\n",
    "    return -(jnp.pi**2) * jnp.sin(jnp.pi * x) * a\n",
    "\n",
    "\n",
    "# exact solution.\n",
    "def u(x, a):\n",
    "    return a * np.sin(np.pi * x)\n",
    "\n",
    "\n",
    "# branch network and trunk network\n",
    "init_br, apply_br = MLP([1, 64, 64])\n",
    "init_tr, apply_tr = MLP([1, 64, 64])\n",
    "\n",
    "\n",
    "# trunk network\n",
    "def tr(params_t, x):\n",
    "    return x * (1 - x) * apply_tr(params_t, jnp.atleast_1d(x)).squeeze()\n",
    "\n",
    "\n",
    "# branch network\n",
    "def br(params_b, a):\n",
    "    return apply_br(params_b, jnp.atleast_1d(a)).squeeze()\n",
    "\n",
    "\n",
    "# deeponet: dot(br, tr)\n",
    "def onet(params, x, a):\n",
    "    params_t, params_b = params\n",
    "    t = tr(params_t, x)\n",
    "    b = br(params_b, a)\n",
    "    return jnp.dot(t, b)\n",
    "\n",
    "\n",
    "# differentiation w.r.t. x\n",
    "def onet_x(params, x, a):\n",
    "    return jax.jacfwd(onet, 1)(params, x, a)\n",
    "\n",
    "\n",
    "def onet_xx(params, x, a):\n",
    "    return jax.jacfwd(onet_x, 1)(params, x, a)\n",
    "\n",
    "\n",
    "# note that we are solving u_xx - f for various f\n",
    "# and f is parameterized with a.\n",
    "def _loss(params, x, a):\n",
    "    return onet_xx(params, x, a) - f(x, a)\n",
    "\n",
    "\n",
    "# Note that the function _loss only takes scalar inputs.\n",
    "# By vmapping, we vectorize it.\n",
    "def loss(params, xx, aa):\n",
    "    pde = jax.vmap(_loss, in_axes=(None, 0, 0))(params, xx, aa)\n",
    "    loss = (pde**2).mean()\n",
    "    return loss\n",
    "\n",
    "\n",
    "nIter = 10000\n",
    "lr = optax.cosine_decay_schedule(1e-3, nIter)\n",
    "opt = optax.adam(lr)\n",
    "\n",
    "# initialize\n",
    "rng, key_b, key_t = jr.split(jr.PRNGKey(0), 3)\n",
    "params_t = init_tr(key_t)\n",
    "params_b = init_br(key_b)\n",
    "params = (params_t, params_b)\n",
    "state = opt.init(params)\n",
    "x = np.linspace(0, 1, 300)\n",
    "a = np.linspace(-1, 1, 100)\n",
    "mesh = np.meshgrid(x, a)\n",
    "xx = mesh[0].ravel()\n",
    "aa = mesh[1].ravel()\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def step(params, state, xx, aa):\n",
    "    v, g = jax.value_and_grad(loss)(params, xx, aa)\n",
    "    updates, state = opt.update(g, state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, state, v\n",
    "\n",
    "\n",
    "loss_total = []\n",
    "for it in (pbar := trange(1, 1 + nIter)):\n",
    "    params, state, v = step(params, state, xx, aa)\n",
    "    if it % 100 == 0:\n",
    "        loss_total.append(v)\n",
    "        pbar.set_postfix({\"loss\": f\"{v:.3e}\"})\n",
    "\n",
    "\n",
    "_, [(ax00, ax01), (ax10, ax11)] = plt.subplots(nrows=2, ncols=2, figsize=(8, 8))\n",
    "# reference sol, prediction\n",
    "x = np.linspace(0, 1, 500)\n",
    "a = np.linspace(-1, 1, 300)\n",
    "mesh = np.meshgrid(x, a, indexing=\"ij\")\n",
    "xx = mesh[0].ravel()\n",
    "aa = mesh[1].ravel()\n",
    "uu = u(*mesh)\n",
    "upred = jax.vmap(onet, in_axes=(None, 0, 0))(params, xx, aa).reshape(500, -1)\n",
    "err = np.abs(upred - uu).sum(0) / np.abs(uu).sum(0)\n",
    "\n",
    "ax00.semilogy(loss_total, label=r\"$\\mathcal{L}_\\mathrm{PINN}$\")\n",
    "ax00.legend()\n",
    "ax00.set_title(\"Physics-informed DeepONet\")\n",
    "ax01.semilogy(a, err, label=\"relative error\")\n",
    "ax01.set_xlabel(r\"$a$\")\n",
    "ax01.set_title(r\"relative $L^1$ as a function of $a$\")\n",
    "\n",
    "uu = uu.ravel()\n",
    "upred = upred.ravel()\n",
    "ax10.tricontourf(xx, aa, uu)\n",
    "ax10.set_title(r\"$u$\")\n",
    "\n",
    "ax11.tricontourf(xx, aa, upred)\n",
    "ax11.set_title(\n",
    "    r\"$u_\\theta$, \" + f\"relative err: {np.abs(upred - uu).sum() / np.abs(uu).sum():.3e}\"\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
