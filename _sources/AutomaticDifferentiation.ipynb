{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Differentiation\n",
    "Automatic differentiation (줄여서 AD)는 함수의 미분 값을 \"자동\"으로 계산해주는 알고리즘입니다.\n",
    "역전파 (backpropagation) 알고리즘으로 알고 계실 수도 있을 것 같습니다.\n",
    "하지만 둘은 다른 용어입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 두 가지 모드\n",
    "AD는 크게 두 가지 방법이 있습니다.\n",
    "Forward mode와 reverse mode입니다.\n",
    "효율적인 계산을 위해서 각 mode가 어떻게 동작하는지 알고, 상황에 따라 적절한 알고리즘을 선택해서 사용할 필요가 있습니다.\n",
    "\n",
    "결론부터 말하자면,\n",
    "함수\n",
    "\\begin{equation*}\n",
    "    f: \\mathbb{R}^{d_\\mathrm{in}} \\rightarrow \\mathbb{R}^{d_\\mathrm{out}}\n",
    "\\end{equation*}\n",
    "가 있을 때\n",
    "{math}`d_\\mathrm{in} < d_\\mathrm{out}`인 경우 forward mode가 빠르고,\n",
    "반대의 경우 {math}`d_\\mathrm{in} > d_\\mathrm{out}`에 reverse mode가 빠릅니다.\n",
    "\n",
    "Deep Learning의 경우 손실 함수 (Loss function) $L$의 input은 neural network의 parameter $\\theta \\in \\mathbb{R}^p$가 되고, output은 손실 함수의 값 $L(\\theta) \\in \\mathbb{R}$이 됩니다.\n",
    "많은 경우에 $p \\gg 1$ 이므로 reverse mode가 빠릅니다.\n",
    "\n",
    "실험을 통해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax  # AD library JAX를 import 합니다.\n",
    "import jax.numpy as jnp  # jnp는 numpy와 \"거의\" 같습니다.\n",
    "\n",
    "\n",
    "def f(x: float):\n",
    "    # scalar를 받아서 [1, x, ..., x^9999]를 리턴하는 함수, d_in < d_out\n",
    "    return jnp.power(x, jnp.arange(10000))\n",
    "\n",
    "\n",
    "def g(y: jnp.ndarray):\n",
    "    # array를 받아서 제곱한 후 평균을 취하는 함수, d_in > d_out\n",
    "    return (y**2).mean()\n",
    "\n",
    "\n",
    "f_fwd = jax.jacfwd(f)\n",
    "f_rev = jax.jacrev(f)\n",
    "g_fwd = jax.jacfwd(g)\n",
    "g_rev = jax.jacrev(g)\n",
    "\n",
    "x = 1.0\n",
    "y = jnp.ones((10000,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "함수 `f`의 경우 $d_\\mathrm{in} = 1 \\ll d_\\mathrm{out} = 10^4$ 이므로, forward mode로 미분한 함수 `f_fwd`가 reverse mode로 미분한 함수 `g_rev`보다 빨라야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.55 ms ± 599 μs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit f_fwd(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.8 ms ± 338 μs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit f_rev(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`f_fwd`가 약 두배 정도 빠른 것을 볼 수 있습니다.\n",
    "\n",
    "반면, 함수 `g`의 경우 $d_\\mathrm{in} = 10^4 \\gg d_\\mathrm{out} = 1$ 이므로, reverse mode로 미분한 함수 `g_rev`가 forward mode로 미분한 함수 `g_fwd` 보다 빨라야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.9 ms ± 101 μs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit g_fwd(y).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.52 ms ± 452 μs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit g_rev(y).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`g_rev`가 약 두배 정도 빠른 것을 볼 수 있습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AD 이해하기\n",
    "컴퓨터로 계산을 할 때, 모든 함수는 간단한 함수의 합성입니다.\n",
    "예를 들어 인공신경망은 덧셈, 곱셈, 그리고 activation function이라는 \"간단한\" 함수들의 합성으로 이루어져 있습니다.\n",
    "AD는 \"간단한\" 함수들의 미분을 미리 계산해두고, $f$를 미분할 때 미리 계산된 미분을 사용하는 방법입니다.\n",
    "\n",
    "어떤 함수 $f: \\mathbb{R}\\rightarrow \\mathbb{R}$가 있고,\n",
    "이 함수는 \"간단한\" 함수들 {math}`f_1, \\dots, f_N: \\mathbb{R}\\rightarrow \\mathbb{R}`의 합성\n",
    "\\begin{equation*}\n",
    "    f = f_N \\circ \\cdots \\circ f_1\n",
    "\\end{equation*}\n",
    "으로 표현할 수 있다고 가정하겠습니다.\n",
    "만약 $f_i$들의 미분을 알고 있다면, 합성함수의 미분법 (Chain rule)을 통해서 $f'$를 계산할 수 있는 공식\n",
    "```{math}\n",
    ":label: chain-rule\n",
    "    f' = \\left(f'_N \\circ f_{N-1} \\circ \\cdots \\circ f_1\\right) \\cdots f'_1\n",
    "```\n",
    "을 얻을 수 있습니다.\n",
    "\n",
    "### Dual numbers\n",
    "Forward mode AD를 잘 이해하기 위해서는 dual number가 무엇인지 알 필요가 있습니다.\n",
    "```{prf:definition}\n",
    ":label: dual-numbers\n",
    "\n",
    "두 실수 $p$ 그리고 $t$가 있다.\n",
    "표현\n",
    "\\begin{equation*}\n",
    "    p + t\\epsilon\n",
    "\\end{equation*}\n",
    "을 dual number라고 한다.\n",
    "\n",
    "두 dual number의 덧셈은\n",
    "\\begin{equation*}\n",
    "    (p_1 + t_1 \\epsilon) + (p_2 + t_2 \\epsilon) = (p_1 + p_2) + (t_1 + t_2)\\epsilon,\n",
    "\\end{equation*}\n",
    "로 정의하고,\n",
    "곱셈은 \n",
    "\\begin{equation*}\n",
    "    (p_1 + t_1 \\epsilon) \\cdot (p_2 + t_2 \\epsilon) = p_1 \\cdot p_2 + (p_1 \\cdot t_2 + p_2 \\cdot t_1)\\epsilon\n",
    "\\end{equation*}\n",
    "으로 정의한다.\n",
    "```\n",
    "먼저 $\\epsilon^2 = 0$을 만족하는 \"표현\" $\\epsilon$을 도입합니다.\n",
    "해당 조건을 만족하는 실수나 복소수는 $0$밖에 없기 때문에, \"표현\"이라고 적었습니다.\n",
    "Dual number는\n",
    "\\begin{equation*}\n",
    "    p + t \\epsilon, \\quad p, t \\in \\mathbb{R}\n",
    "\\end{equation*}\n",
    "($p$, $t$는 각각 primal, tangent의 첫 글자입니다.)\n",
    "덧셈과 곱셈은 자연스럽게 정의됩니다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mas557",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
