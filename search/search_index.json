{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>Physics-informed neural networks (PINNs)\ub294 2018\ub144 11\uc6d4\uc5d0 \"The journal of computational physics\"\uc5d0 \uc628\ub77c\uc778 \ubc84\uc804\uc73c\ub85c \ucc98\uc74c \ub4f1\uc7a5\ud588\uc2b5\ub2c8\ub2e4<sup>1</sup>. \uc774\ub294 neural networks \ubaa8\ub378\uc5d0 \ubbf8\ubd84\ubc29\uc815\uc2dd \uad00\ub828 \uc815\ubcf4\ub97c \ucd94\uac00\ud558\ub294 \ubc29\ubc95\uc73c\ub85c, \uace7\uc774\uc5b4 \uc77c\ubc18\uc801\uc778 \uba38\uc2e0\ub7ec\ub2dd \ubaa8\ub378\uc5d0 \ubb3c\ub9ac\uc801 \uc815\ubcf4\ub97c \ucd94\uac00\ud558\ub294 framework\uc778 physics-informed machine learning (PIML)<sup>2</sup>\uc73c\ub85c \uc77c\ubc18\ud654\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \uc774 \uc5f0\uad6c\ub294 \uc804\uc138\uacc4\uc801\uc73c\ub85c \ub9ce\uc740 \uad00\uc2ec\uc744 \ubc1b\uc558\uc2b5\ub2c8\ub2e4. \uc81c\uac00 PINNs \ub17c\ubb38\uc744 \ucc98\uc74c\uc73c\ub85c \uc811\ud55c 2022\ub144 11\uc6d4\uc5d0, \uc778\uc6a9 \ud69f\uc218\ub294 5000\ud68c \uc815\ub3c4 \uc600\uc2b5\ub2c8\ub2e4 (\uc778\uc6a9 \ud69f\uc218 \uae30\uc900\uc73c\ub85c gPC \ub17c\ubb38\ubcf4\ub2e4 \uc544\ub798\uc5d0 \uc788\uc5c8\uc74c). 2024\ub144 7\uc6d4 3\uc77c Google Scholar \uae30\uc900\uc73c\ub85c \uc778\uc6a9 \ud69f\uc218\ub294 \uac70\uc758 \ub450\ubc30\uac00 \ub418\uc5c8\uc2b5\ub2c8\ub2e4 (9621). \ud558\uc9c0\ub9cc \uc548\ud0c0\uae5d\uac8c\ub3c4 \ud55c\uae00\ub85c \ub41c \uc790\ub8cc\uac00 \uc544\uc9c1 \ub9ce\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.</p> <p>\uc774 \ucc45\uc758 \ubaa9\uc801\uc740 \ud55c\uae00\ub85c \ub41c physics-informed neural networks \uac15\uc758\ub97c \ub9cc\ub4dc\ub294 \uac83\uc785\ub2c8\ub2e4.</p> <p>\uac15\uc758\uc758 \uc81c\ubaa9\uc740 \uc81c\uac00 \ucc98\uc74c \uba38\uc2e0\ub7ec\ub2dd\uc744 \uc811\ud558\uac8c \ub41c \uacc4\uae30\uc778 \ubaa8\ub450\ub97c \uc704\ud55c \uba38\uc2e0\ub7ec\ub2dd\uc5d0\uc11c \ub530\uc654\uc2b5\ub2c8\ub2e4.</p>"},{"location":"#programming-language-python","title":"Programming Language: Python","text":"<p>\ub300\ubd80\ubd84\uc758 \uba38\uc2e0\ub7ec\ub2dd \uc5f0\uad6c\ub294 Python, \uadf8 \uc911\uc5d0\uc11c\ub3c4 PyTorch\ub97c \uae30\ubc18\uc73c\ub85c \uc774\ub8e8\uc5b4\uc9d1\ub2c8\ub2e4. Julia\uc640 \uac19\uc740 \ub354 \ucd5c\uadfc language\ub97c \uc0ac\uc6a9\ud560 \uc218\ub3c4 \uc788\uaca0\uc9c0\ub9cc, \uc544\ubb34\ub798\ub3c4 \uc0ac\uc6a9 \ubc29\ubc95\uc774 Python \ub9cc\ud07c \uac04\ub2e8\ud558\uc9c0\ub294 \uc54a\uc2b5\ub2c8\ub2e4. \ub530\ub77c\uc11c \ubcf8 \uac15\uc758\uc5d0\uc11c\ub294 Python\uc744 \uc8fc\ub41c \ud504\ub85c\uadf8\ub798\ubc0d \uc5b8\uc5b4\ub85c \uc0ac\uc6a9\ud569\ub2c8\ub2e4.</p>"},{"location":"#machine-learning-library-jax","title":"Machine Learning Library: JAX","text":"<p>PyTorch\uac00 \uba38\uc2e0 \ub7ec\ub2dd \uc5f0\uad6c\uc758 \ud070 \ubd80\ubd84\uc744 \ucc28\uc9c0\ud558\uace0 \uc788\uc9c0\ub9cc, \uc801\uc5b4\ub3c4 PINNs\uc5d0 \ub300\ud574\uc11c\ub294 JAX\uc5d0 \ube44\ud574 \ubd80\uc871\ud55c \ubd80\ubd84\uc774 \uc788\uc2b5\ub2c8\ub2e4. \ubc14\ub85c forward mode automatic differentiation (AD)\uacfc just-in-time (JIT) compilation \uc785\ub2c8\ub2e4. PyTorch 2.0\uc5d0 \ub4e4\uc5b4\uc624\uba74\uc11c jvp \ud568\uc218\uc640 compile \ud568\uc218\uac00 \uc0dd\uae30\uba74\uc11c \uc704 \ub450 \uae30\ub2a5\uc5d0 \ub300\ud55c PyTorch \uc720\uc800\uc758 \uac08\uc99d\uc774 \uc5b4\ub290 \uc815\ub3c4 \ud574\uc18c\uac00 \ub418\uc5c8\uc2b5\ub2c8\ub2e4. \uadf8\ub7ec\ub098 \uc544\uc9c1 Taylor mode AD \uac19\uc740 \uc870\uae08 \ub354 \uace0\uae09 \ud234\uc740 \uc9c0\uc6d0\uc774 \ub418\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.</p> <p>\ud559\uc2b5 \uc18d\ub3c4\ub97c \uac1c\uc120\ud558\ub294\ub370 \uc788\uc5b4 forward mode AD\uac00 \uc0c1\ub2f9\ud788 \uc911\uc694\ud55c \uc5ed\ud560\uc744 \ud558\ub294 \ub9cc\ud07c, \ubcf8 \uac15\uc758\uc5d0\uc11c\ub294 \uba38\uc2e0 \ub7ec\ub2dd \ub77c\uc774\ube0c\ub7ec\ub9ac\ub85c JAX\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4.</p>"},{"location":"#jax","title":"JAX \uad00\ub828 \ubb38\uc11c\ub4e4","text":"<ul> <li>JAX \uc124\uce58</li> <li>Optax - Gradient Descent based Optimizers</li> <li>JAXopt - General Optimizers</li> </ul>"},{"location":"#etc","title":"Etc","text":"<ul> <li>email</li> <li>\uc5b4\ub5a4 \uc885\ub958\uc758 PR\ub3c4 \ud658\uc601\ud569\ub2c8\ub2e4.</li> </ul> <ol> <li> <p>Raissi, M., Perdikaris, P., &amp; Karniadakis, G.E. (2019) Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational physics, 378, 686\u2013707.\u00a0\u21a9</p> </li> <li> <p>Karniadakis, G.E., Kevrekidis, I.G., Lu, L., Perdikaris, P., Wang, S., &amp; Yang, L. (2021) Physics-informed machine learning. Nature Reviews Physics, 3, 422\u2013440.\u00a0\u21a9</p> </li> </ol>"},{"location":"1.%20Preliminaries/","title":"Preliminaries","text":"<p>\ubcf8 \ucc55\ud130\uc5d0\uc11c\ub294 physics-informed neural networks (PINNs)\ub97c \uacf5\ubd80\ud558\ub294\ub370 \ud544\uc694\ud55c \ubc30\uacbd \uc9c0\uc2dd\uc744 \ub2e4\ub8f9\ub2c8\ub2e4.</p> <p>PINNs\ub294 \uc778\uacf5\uc2e0\uacbd\ub9dd \uae30\ubc18 \ubc29\ubc95\uc5d0 differential equation\uc758 \ud615\ud0dc\ub85c \uc8fc\uc5b4\uc9c4 \uc815\ubcf4\ub97c \ub354\ud558\ub294 \ubc29\ubc95\uc785\ub2c8\ub2e4. \uadf8\ub7ec\ubbc0\ub85c, \uc778\uacf5\uc2e0\uacbd\ub9dd\uacfc differential equation\uc5d0 \ub300\ud55c \ubc30\uacbd \uc9c0\uc2dd\uc774 \uaf2d \ud544\uc694\ud569\ub2c8\ub2e4.</p>"},{"location":"1.%20Preliminaries/1.NeuralNetworks/","title":"Neural Networks","text":"<p>\uc778\uacf5\uc2e0\uacbd\ub9dd (neural networks)\uc740 affine transformation, \uadf8\ub9ac\uace0 nonlinear activation function\uc758 \ubc18\ubcf5\ub41c \ud569\uc131\uc785\ub2c8\ub2e4. Affine transformation\uc758 \ud69f\uc218\ub97c depth, affine transformation\uc758 output dimension\uc744 width\ub77c\uace0 \ud569\ub2c8\ub2e4. \uc774 \ud568\uc218\ub97c fully connected neural networks (FNNs) \ud639\uc740 multi-layer perceptrons (MLPs)\ub77c\uace0 \ubd80\ub985\ub2c8\ub2e4.</p>"},{"location":"1.%20Preliminaries/1.NeuralNetworks/#definition-neural-networks","title":"Definition: neural networks","text":"<p>\uc544\ud0a4\ud14d\uccd0 \\([n_0, n_1, \\dots, n_d]\\), activation function \\(\\phi\\)\uac00 \uc788\uc744 \ub54c, MLP\ub294 \ub2e4\uc74c \ud568\uc218\ub85c \uc815\uc758\ud569\ub2c8\ub2e4. $$     \\mathrm{MLP}(x;\\theta) = A^d \\circ \\left( \\phi \\circ A^{d-1} \\right) \\circ \\cdots \\circ \\left( \\phi \\circ A^1 \\right)(x). $$ \uc5ec\uae30\uc11c \\(A^i(h) = W^i h + b^i\\)\ub85c, \\(W^i \\in \\mathbb{R}^{n_i \\times n_{i-1}}\\) \uadf8\ub9ac\uace0 \\(b^i \\in \\mathbb{R}^{n_i}\\) \uc785\ub2c8\ub2e4. \ub610\ud55c activation function\uc740 component-wise\ud558\uac8c \uacc4\uc0b0\ud569\ub2c8\ub2e4. \\(\\theta\\)\ub294 \ub124\ud2b8\uc6cc\ud06c \ud30c\ub77c\ubbf8\ud130\ub4e4\uc758 \uc9d1\ud569 \\(\\{(W^i, b^i)|i=1, \\dots, d\\}\\)\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.</p> <p>Remark</p> <p>Affine transformation \\(A^i(h) = W^i h + b^i\\)\ub294 matrix \\(W^i\\)\uc640 vector \\(h\\)\uc758 \uacf1\uc744 \ud3ec\ud568\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uc774 matrix-vector product\uc758 \uacc4\uc0b0 \ubcf5\uc7a1\ub3c4\ub294 \\(\\mathcal{O}(n_i n_{i-1})\\) \uc785\ub2c8\ub2e4. \uc774\ub294 graphical processing unit (GPU)\uc5d0\uc11c \uad49\uc7a5\ud788 \ube60\ub974\uac8c \uacc4\uc0b0\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc65c large scale deep learning\uc5d0\uc11c GPU\uac00 \uaf2d \ud544\uc694\ud55c\uc9c0, \uc65c checkpointing \uac19\uc740 \ubc29\ubc95\uc5d0\uc11c affine hidden layer output\ub4e4\uc744 \uc800\uc7a5\ud558\ub294\uc9c0 \ub4f1\uc758 \uc774\uc720\uac00 \uc5ec\uae30\uc5d0 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>\ud604\ub300\uc801 deep learning\uc5d0\uc11c\ub294 \ub354 \ubcf5\uc7a1\ud558\uace0 \uac70\ub300\ud55c \uad6c\uc870\uc758 neural network\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \ud558\uc9c0\ub9cc \ubcf8 \ucc45\uc5d0\uc11c \ub2e4\ub8f0 \ubb38\uc81c\ub4e4\uc5d0 \uc788\uc5b4\uc11c\ub294 MLP\ub9c8\uc800 polynomial-based \ubc29\ubc95\ub4e4\ubcf4\ub2e4 \ud6e8\uc52c model\uc758 \ubcf5\uc7a1\ub3c4\uac00 \ud06c\uace0, \ub290\ub9bd\ub2c8\ub2e4.</p>"},{"location":"1.%20Preliminaries/1.NeuralNetworks/#universal-approximation-theorem","title":"Universal Approximation Theorem","text":"<p>Universal approximation theorem.</p> <p>\uc5b4\ub5a4 compact domain \\(\\Omega\\)\uc5d0\uc11c \uc815\uc758\ub41c \uc5f0\uc18d\ud568\uc218 \\(f\\)\uac00 \uc788\uace0 \uc5d0\ub7ec \ub808\ubca8 \\(\\epsilon\\)\uc774 \uc8fc\uc5b4\uc84c\uc744 \ub54c $$     \\sup_{x\\in \\Omega}|\\mathrm{MLP}(x;\\theta) - f(x)| &lt; \\epsilon $$ \uc744 \ub9cc\uc871\ud558\ub294 \\(\\theta\\)\ub97c \ud56d\uc0c1 \ucc3e\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>Remark</p> <p>\uac04\ud639 universal approximation theorem\uc774 \uc778\uacf5\uc2e0\uacbd\ub9dd\uc774 \uc798 \ub3d9\uc791\ud558\ub294 \uc774\uc720\uac00 \ub41c\ub2e4\ub294 \uc124\uba85\uc744 \ub4e3\uace4 \ud569\ub2c8\ub2e4. \ud558\uc9c0\ub9cc \uc774\ub294 \ud2c0\ub9b0 \uc124\uba85\uc785\ub2c8\ub2e4. \uc0ac\uc2e4 polynomial\ub4e4\ub3c4 universal approximation theorem\uc744 \ub9cc\uc871\ud569\ub2c8\ub2e4. \ub9cc\uc57d \uc704 \uc124\uba85\uc774 \ub9de\ub2e4\uba74, polynomial-based \ubaa8\ub378\ub4e4\ub3c4 neural network \ub9cc\ud07c \ud37c\ud3ec\uba3c\uc2a4\ub97c \ub0b4 \uc918\uc57c \ud569\ub2c8\ub2e4. Universal approximation theorem\uc740 \ub2e8\uc9c0 \ucd5c\uc18c\ud55c\uc758 \uc774\ub860\uc801\uc778 \ubcf4\uc7a5\uc77c \ubfd0\uc785\ub2c8\ub2e4.</p>"},{"location":"1.%20Preliminaries/2.DifferentialEquations/","title":"Differential Equation","text":"<p>\\(I = [-1, 1]\\)\uc704\uc5d0 \ud568\uc218 \\(T_0(x)\\)\uac00 \uc8fc\uc5b4\uc838 \uc788\uc2b5\ub2c8\ub2e4. \\(T_0(-1) = T_0(1) = 0\\), \uadf8\ub9ac\uace0 \\(x \\ne 0\\)\uc5d0 \ub300\ud574 \\(T_0(x) &gt; 0\\)\ub97c \ub9cc\uc871\ud55c\ub2e4\uace0 \ud558\uaca0\uc2b5\ub2c8\ub2e4. \uc774\ub294 \uae38\uc774\uac00 \\(2\\)\uc778 \uc1e0\ub9c9\ub300\uae30\uc758 \uc628\ub3c4\ub97c \ud568\uc218\ub85c \ud45c\ud604\ud55c \uac83\uc73c\ub85c \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4. Heat equation (\uc5f4 \ubc29\uc815\uc2dd)\uc740 \uc2dc\uac04\uc774 \uc9c0\ub0a8\uc5d0 \ub530\ub77c \uc1e0\ub9c9\ub300\uae30 \uc628\ub3c4\uac00 \uc5b4\ub5bb\uac8c \ubcc0\ud558\ub294\uc9c0\ub97c \uc11c\uc220\ud569\ub2c8\ub2e4.</p>"},{"location":"1.%20Preliminaries/2.DifferentialEquations/#definition-heat-equation","title":"Definition - Heat Equation","text":"<p>Heat equation\uc740 \ub2e4\uc74c\uacfc \uac19\uc774 \uc815\uc758\ud569\ub2c8\ub2e4.</p> \\[     \\frac{\\partial T}{\\partial t}(x, t) = \\frac{\\partial^2 T}{\\partial x^2}(x, t) \\quad (x,t) \\in I \\times (0, \\infty). \\] <p>\ub9cc\uc57d \\(T(-1,t) = T(1, t) = 0\\)\ub77c\ub294 boundary condition, \uadf8\ub9ac\uace0 \\(T(x, 0) = T_0(x)\\)\ub77c\ub294 \uc870\uac74\uc744 \uace0\ub824\ud55c\ub2e4\uba74, \uc704 \uc5f4 \ubc29\uc815\uc2dd\uc740 \uc1e0 \ub9c9\ub300\uae30 \ub05d\uc758 \uc628\ub3c4\uac00 \\(0\\)\uc73c\ub85c \uace0\uc815, \uc678\ubd80\uc640 \uc628\ub3c4\ub97c \uc8fc\uace0\ubc1b\uc9c0 \uc54a\uc73c\uba70, \uc2dc\uac04\uc774 \\(0\\)\uc77c \ub54c \uc628\ub3c4\uc758 \ubd84\ud3ec\uac00 \\(T_0(x)\\)\ub85c \uc8fc\uc5b4\uc9c4 \uc1e0 \ub9c9\ub300\uae30\uc758 \\(t\\)\ucd08 \ud6c4 \uc628\ub3c4 \ubd84\ud3ec\ub97c \uc608\uce21\ud558\ub294 \ubc29\uc815\uc2dd\uc774 \ub429\ub2c8\ub2e4. \uc88c\ubcc0\uc5d0\ub294 \uc628\ub3c4 \ubd84\ud3ec\uc758 \uc2dc\uac04\uc5d0 \ub530\ub978 \ubcc0\ud654\uac00, \uadf8\ub9ac\uace0 \uc6b0\ubcc0\uc5d0\ub294 \uc628\ub3c4 \ubd84\ud3ec\uc758 \uacf5\uac04\uc5d0 \ub530\ub978 \ubbf8\ubd84\uc774 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ub4f1\uc2dd\uc744 \ubbf8\ubd84\ubc29\uc815\uc2dd (\ud3b8\ubbf8\ubd84\uc774 \uc788\uc73c\ubbc0\ub85c \ud3b8\ubbf8\ubd84\ubc29\uc815\uc2dd)\uc774\ub77c\uace0 \ud558\uace0, \ubbf8\ubd84\ubc29\uc815\uc2dd\uc744 \ub9cc\uc871\ud558\ub294 \\(T\\)\ub97c \ubbf8\ubd84\ubc29\uc815\uc2dd\uc758 \ud574\ub77c\uace0 \ud569\ub2c8\ub2e4.</p>"},{"location":"1.%20Preliminaries/3.AutoDiff/","title":"Automatic Differentiation","text":"<p>\ucef4\ud4e8\ud130\ub85c \uacc4\uc0b0\uc744 \ud560 \ub54c, \ubaa8\ub4e0 \ud568\uc218\ub294 \uac04\ub2e8\ud55c \ud568\uc218\uc758 \ud569\uc131\uc785\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4 \uc778\uacf5\uc2e0\uacbd\ub9dd\uc740 \ub367\uc148, \uacf1\uc148, \uadf8\ub9ac\uace0 activation function\uc774\ub77c\ub294 \"\uac04\ub2e8\ud55c\" \ud568\uc218\ub4e4\uc758 \ud569\uc131\uc73c\ub85c \uc774\ub8e8\uc5b4\uc838 \uc788\uc2b5\ub2c8\ub2e4. Automatic Differentiation (AD)\ub294 \"\uac04\ub2e8\ud55c\" \ud568\uc218\ub4e4\uc758 \ubbf8\ubd84\uc744 \ubbf8\ub9ac \uacc4\uc0b0\ud574\ub450\uace0, \\(f\\)\ub97c \ubbf8\ubd84\ud560 \ub54c \ubbf8\ub9ac \uacc4\uc0b0\ub41c \ubbf8\ubd84\uc744 \uc0ac\uc6a9\ud558\ub294 \ubc29\ubc95\uc785\ub2c8\ub2e4.</p> <p>Remark</p> <p>\uc5ed\uc804\ud30c (backpropagation) \uc54c\uace0\ub9ac\uc998\uc740 reverse-mode AD\ub97c \uc9c0\uce6d\ud569\ub2c8\ub2e4.</p> <p>\uc5b4\ub5a4 \ud568\uc218 \\(f: \\mathbb{R}\\rightarrow \\mathbb{R}\\)\uac00 \uc788\uace0, \uc774 \ud568\uc218\ub294 \"\uac04\ub2e8\ud55c\" \ud568\uc218\ub4e4 \\(f_1, \\dots, f_N: \\mathbb{R}\\rightarrow \\mathbb{R}\\)\uc758 \ud569\uc131 $$     f = f_N \\circ \\cdots \\circ f_1 $$ \uc73c\ub85c \ud45c\ud604\ud560 \uc218 \uc788\ub2e4\uace0 \ud558\uaca0\uc2b5\ub2c8\ub2e4. \ub9cc\uc57d \\(f_i\\)\ub4e4\uc758 \ubbf8\ubd84\uc744 \uc54c\uace0 \uc788\ub2e4\uba74, \ud569\uc131\ud568\uc218\uc758 \ubbf8\ubd84\ubc95 (Chain rule)\uc744 \ud1b5\ud574\uc11c \\(f'\\)\ub97c \uacc4\uc0b0\ud560 \uc218 \uc788\ub294 \uacf5\uc2dd $$     f' = \\left(f_N' \\circ f_{N-1} \\circ \\cdots \\circ f_1\\right) \\cdots f'_1 $$ \uc744 \uc5bb\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p>"},{"location":"1.%20Preliminaries/3.AutoDiff/#tl-dr","title":"TL DR","text":"<p>AD\ub294 \ud06c\uac8c \ub450 \uac00\uc9c0 \ubc29\ubc95\uc774 \uc788\uc2b5\ub2c8\ub2e4. Forward mode\uc640 reverse mode\uc785\ub2c8\ub2e4. \uc0c1\ud669\uc5d0 \ub530\ub77c \uc801\uc808\ud55c mode\ub97c \uc120\ud0dd\ud574\uc11c \uc0ac\uc6a9\ud55c\ub2e4\uba74, \ud6a8\uc728\uc801\uc778 \ucef4\ud4e8\ud305\uc744 \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>\uacb0\ub860\ubd80\ud130 \ub9d0\ud558\uc790\uba74, \ud568\uc218 $$     f: \\mathbb{R}^{d_\\mathrm{in}} \\rightarrow \\mathbb{R}^{d_\\mathrm{out}} $$ \uac00 \uc788\uc744 \ub54c \\(d_\\mathrm{in} &lt; d_\\mathrm{out}\\)\uc778 \uacbd\uc6b0 forward mode\uac00 \ube60\ub974\uace0, \ubc18\ub300\uc758 \uacbd\uc6b0 \\(d_\\mathrm{in} &gt; d_\\mathrm{out}\\)\uc5d0 reverse mode\uac00 \ube60\ub985\ub2c8\ub2e4.</p> <p>Deep Learning\uc758 \uacbd\uc6b0 \uc190\uc2e4 \ud568\uc218 (Loss function) \\(L\\)\uc758 input\uc740 neural network\uc758 parameter \\(\\theta \\in \\mathbb{R}^p\\)\uac00 \ub418\uace0, output\uc740 \uc190\uc2e4 \ud568\uc218\uc758 \uac12 \\(L(\\theta) \\in \\mathbb{R}\\)\uc774 \ub429\ub2c8\ub2e4. \ub9ce\uc740 \uacbd\uc6b0\uc5d0 \\(p \\gg 1\\) \uc774\ubbc0\ub85c reverse mode\uac00 \ube60\ub985\ub2c8\ub2e4.</p>"},{"location":"1.%20Preliminaries/3.AutoDiff/#dual-numbers","title":"Dual numbers","text":"<p>Forward mode AD\ub97c \uc798 \uc774\ud574\ud558\uae30 \uc704\ud574\uc11c\ub294 dual number\uac00 \ubb34\uc5c7\uc778\uc9c0 \uc54c \ud544\uc694\uac00 \uc788\uc2b5\ub2c8\ub2e4.</p>"},{"location":"1.%20Preliminaries/3.AutoDiff/#definition-dual-numbers","title":"Definition - Dual numbers","text":"<p>\\(\\epsilon^2 = 0\\)\uc778 \uc218 \\(\\epsilon\\)\uc774 \uc788\uc2b5\ub2c8\ub2e4. \uadf8\ub9ac\uace0 \ub450 \uc2e4\uc218 \\(p\\) \uadf8\ub9ac\uace0 \\(t\\)\uac00 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ub54c \ud45c\ud604 $$     p + t\\epsilon $$ \uc744 dual number\ub77c\uace0 \ud569\ub2c8\ub2e4.</p> <p>\ub450 dual number\uc758 \ub367\uc148\uc740 $$     (p_1 + t_1 \\epsilon) + (p_2 + t_2 \\epsilon) = (p_1 + p_2) + (t_1 + t_2)\\epsilon, $$ \ub85c \uc815\uc758\ud558\uace0, \uacf1\uc148\uc740  $$     (p_1 + t_1 \\epsilon) \\cdot (p_2 + t_2 \\epsilon) = p_1 \\cdot p_2 + (p_1 \\cdot t_2 + p_2 \\cdot t_1)\\epsilon $$ \uc73c\ub85c \uc815\uc758\ud569\ub2c8\ub2e4.</p>"},{"location":"1.%20Preliminaries/3.AutoDiff/#example-dual-numbers","title":"Example - Dual numbers","text":"<p>\ub450 dual numbers \\(p_1 + t_1 \\epsilon\\), \uadf8\ub9ac\uace0 \\(p_2 + t_2 \\epsilon\\)\uc774 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ub54c \ub450 \uc218\uc758 \ube84\uc148\uacfc \ub098\ub217\uc148\uc744 \uacc4\uc0b0\ud574 \ubd05\uc2dc\ub2e4.</p> <p>\uc0ac\uc2e4, \uc704 \uc815\uc758\ub294 \ub2e4\uc74c \uc815\uc758\uc758 \ud2b9\ubcc4\ud55c \uacbd\uc6b0\uc785\ub2c8\ub2e4.</p> <p>\ud568\uc218 \\(f\\)\uac00 \uc788\uc744 \ub54c, $$     f(p + t\\epsilon) = f(p) + \\partial f(p)[t]\\epsilon $$ \uc73c\ub85c \uc815\uc758\ud569\ub2c8\ub2e4.</p> <p>\\(\\epsilon\\)\uc758 \uacc4\uc218\uac00 directional derivative\ub97c \ub098\ud0c0\ub0b4\uace0 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>\ud568\uc218 \\(f\\) \uadf8\ub9ac\uace0 \\(g\\)\uac00 \uc788\uc744 \ub54c, \\((f \\circ g) ( p + t\\epsilon)\\)\uc744 \uacc4\uc0b0\ud574 \ubd05\uc2dc\ub2e4.</p> <p>\uc704\uc5d0\uc11c \ubaa8\ub4e0 \ud568\uc218\ub294 \"\uac04\ub2e8\ud55c\" \ud568\uc218\ub4e4\uc758 \ud569\uc131\uc73c\ub85c \uc774\ub8e8\uc5b4\uc9c4\ub2e4\uace0 \ud588\uc5c8\uc2b5\ub2c8\ub2e4. \uc5ec\uae30\uc11c \"\uac04\ub2e8\ud55c\" \ud568\uc218\ub780, dual number\ub97c \ud1b5\ud574 \ud45c\ud604\ud560 \uc218 \uc788\ub294 \ud568\uc218\ub97c \uc758\ubbf8\ud569\ub2c8\ub2e4. (Operator overloading) AD\ub294 \ud568\uc218 \\(f = f_N \\circ \\cdots \\circ f_1\\)\uac00 \uc788\uc744 \ub54c, \\(f_i\\)\ub4e4\uc744 \ubaa8\ub450 dual number\ub97c \ud1b5\ud574 \ud45c\ud604\ud55c \ud6c4, \ud568\uc218\uc758 \ud569\uc131\uc744 \uacc4\uc0b0\ud574\uc11c \\(f(p)\\)\uc640 \\(\\partial f(p)\\)\ub97c \uc5bb\ub294 \ubc29\ubc95\uc785\ub2c8\ub2e4.</p>"},{"location":"1.%20Preliminaries/3.AutoDiff/#forward-vs-reverse","title":"Forward vs Reverse","text":"<p>\uc774\uc81c forward, \uadf8\ub9ac\uace0 reverse mode AD\ub97c \uc54c\uc544\ubcf4\uace0 \ube44\uad50\ud574 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4.</p>"},{"location":"1.%20Preliminaries/3.AutoDiff/#forward-mode-ad","title":"Forward mode AD","text":"<p>\\(\\partial f(p) t\\)\ub294 \\(p\\)\uc5d0\uc11c \\(f\\)\uc758 Jacobian\uc744 \uacc4\uc0b0\ud55c \ud6c4, tangent vector \\(t\\)\ub97c \uacf1\ud55c \ud615\ud0dc\uc785\ub2c8\ub2e4. \\(f_1(p) + \\partial f_1 (p)t\\)\ub97c \\(f_2\\)\uc5d0 \uc9d1\uc5b4\ub123\uc5b4\uc11c primal\uacfc tangent vector\ub97c \uacc4\uc0b0\ud558\uace0, \uc774\ub97c \ub2e4\uc2dc \\(f_3\\)\uc5d0 \ub123\uace0, ... \ub97c \ubc18\ubcf5\ud569\ub2c8\ub2e4. \uac01 \uacfc\uc815\uc5d0\uc11c tangent vector\uc758 \uacc4\uc0b0\uc740 Jacobian\uacfc \uc804 \ub2e8\uacc4\uc5d0\uc11c \uacc4\uc0b0\ud55c tangent vector \uc0ac\uc774\uc758 matrix-vector product\uc785\ub2c8\ub2e4. \uacb0\ub860\uc801\uc73c\ub85c, $$     \\partial f(p)t = \\partial f_N \\cdots \\partial f_1(p) t $$ \ub85c \ud45c\ud604\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\uac83\uc744 \uc885\uc885 Jacobian-vector product (JVP)\ub85c \ubd80\ub985\ub2c8\ub2e4.</p>"},{"location":"1.%20Preliminaries/3.AutoDiff/#reverse-mode-ad","title":"Reverse mode AD","text":"<p>\\(d_\\mathrm{in} \\gg d_\\mathrm{out}\\)\uc758 \uacbd\uc6b0\uc5d0 forward mode AD\ub294 \ud6a8\uc728\uc801\uc774\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4. \\(\\partial f(p)\\)\uc740 \\(d_\\mathrm{out}\\)\uac1c\uc758 \uc5f4\uacfc \\(d_\\mathrm{in}\\)\uac1c\uc758 \ud589\uc744 \uac00\uc9c0\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \uacbd\uc6b0 \\(\\partial f(p) t\\)\ub294 computationally expensive\ud558\uc9c0\ub9cc, \\(t^T \\partial f(p)\\)\ub294 computationally \uc800\ub834\ud569\ub2c8\ub2e4. \\(t^T\\)\ub294 cotangent vector\ub77c\uace0 \ubd80\ub974\uba70, \\(t^T \\partial f(p)\\)\ub97c \uacc4\uc0b0\ud558\ub294 \uac83\uc744 reverse mode AD\ub77c\uace0 \ubd80\ub985\ub2c8\ub2e4. \ud480\uc5b4\ud5e4\uccd0\ubcf4\uba74 $$     t^T \\partial f(p) = t^T \\partial f_N \\cdots f_1(p) $$ \uc785\ub2c8\ub2e4. \uc774\uac83\uc744 \uc885\uc885 vector-Jacobian product (VJP)\ub85c \ubd80\ub985\ub2c8\ub2e4. \uc5b4\ub5bb\uac8c \\(t^T \\partial f_N\\)\ub97c \ub9e8 \ucc98\uc74c \uacc4\uc0b0\ud558\ub294\uc9c0\uc5d0 \ub300\ud574 \uc758\ubb38\uc774 \ub4e4\uc5b4\uc57c \ud569\ub2c8\ub2e4. \\(\\partial f_N\\)\uc758 primal value\ub294 \\((f_{N-1} \\circ \\dots \\circ f_1)(p)\\)\uc774\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4.</p> <p>Reverse mode AD\ub294 \uba3c\uc800 \\(f(p)\\)\ub97c evaluate \ud558\uba74\uc11c \\(f_i(p)\\) \uac12\ub4e4\uc744 \ubaa8\ub450 \uc800\uc7a5\ud574 \ub450\uace0, \ub098\uc911\uc5d0 vector-Jacobian product\ub97c \uacc4\uc0b0\ud560 \ub54c \uaebc\ub0b4\uc11c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \\(d_\\mathrm{in} \\gg d_\\mathrm{out}\\) \uacbd\uc6b0\uc5d0 \uacc4\uc0b0 \uc18d\ub3c4\ub294 forward mode \ubcf4\ub2e4 \ud6e8\uc52c \ube60\ub974\uc9c0\ub9cc, \ub300\uc2e0 \uba54\ubaa8\ub9ac \ube44\uc6a9\uc774 \ub192\uc2b5\ub2c8\ub2e4.</p>"},{"location":"1.%20Preliminaries/4.Errors/","title":"Three types of errors","text":"<p>\uc138\uc0c1\uc758 \ub9ce\uc740 \uc77c\ub4e4\uc740 \ubd80\ubd84\uc801\uc73c\ub85c \uc54c\ub824\uc9c4 \ud568\uc218 \\(f\\)\ub97c \uc798 \uc54c\ub824\uc9c4 \ud568\uc218 \\(g\\)\ub85c \ub300\uccb4\ud558\uace0\uc790 \ud558\ub294 \uacfc\uc815\uc785\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4 \\(f\\)\uac00 \uc774\uc804 \ub300\ud654 \ub0b4\uc6a9\uc744 \uc785\ub825\uc73c\ub85c, \ub2f5\ubcc0\uc744 \ucd9c\ub825\uc73c\ub85c \ub0b4\ub193\ub294 \ud568\uc218\ub77c\uace0 \ud558\uaca0\uc2b5\ub2c8\ub2e4. (\ucd9c\ub825 \uac12\uc774 \uc720\uc77c\ud558\uc9c0 \uc54a\uc73c\ubbc0\ub85c \uc218\ud559\uc801\uc73c\ub85c \ud568\uc218\ub294 \uc544\ub2d9\ub2c8\ub2e4.) OpenAI\ub294 \ud568\uc218 \\(f\\)\ub97c \uc704\ud574 Chat-GPT\ub97c \\(g\\)\ub85c\uc368 \uc81c\uc548\ud55c \uc148\uc785\ub2c8\ub2e4.</p> <p>\ud568\uc218 \\(g\\)\ub97c \ucc3e\ub294 \uacfc\uc815\uc5d0\uc11c \ud06c\uac8c \uc138 \uac00\uc9c0 \uc885\ub958\uc758 \uc5d0\ub7ec\uac00 \ubc1c\uc0dd\ud569\ub2c8\ub2e4. (PINNs \uc785\uc7a5\uc5d0\uc11c) \uc911\uc694\ud55c \uc21c\uc11c\ub300\ub85c \ub098\uc5f4\ud558\uaca0\uc2b5\ub2c8\ub2e4.</p> <ul> <li>Optimization error</li> <li>Estimation error</li> <li>Approximation error</li> </ul> <p>\ubcf8 \uc7a5\uc5d0\uc11c\ub294 \uac01 error\uc758 (informal) \uc815\uc758\uc640 \uc758\uc758\ub97c \ub2e4\ub8f9\ub2c8\ub2e4.</p>"},{"location":"1.%20Preliminaries/4.Errors/#approximation-error","title":"Approximation error","text":"<p>\ud568\uc218 \\(g\\)\ub294 \ubcf4\ud1b5 \uc5b4\ub5a4 \uc9d1\ud569 \\(\\mathcal{G}\\) \uc548\uc5d0 \uc18d\ud574 \uc788\uc2b5\ub2c8\ub2e4. Fully connected neural network\ub97c \uc608\ub85c \ub4e4\uc5b4\ubcf4\uaca0\uc2b5\ub2c8\ub2e4. Network\uc758 \ub113\uc774 (width), \uae4a\uc774 (depth), \uadf8\ub9ac\uace0 activation function\uc774 \uc815\ud574\uc9c0\uace0 \ub098\uba74, network parameter \\(\\theta\\)\uac00 \ud568\uc218\ub97c \uacb0\uc815\ud569\ub2c8\ub2e4. \uc774 \ub54c \\(\\mathcal{G} = \\{g_\\theta: \\theta\\in \\mathbb{R}^p\\}\\)\ub77c\uace0 \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>Approximation error\ub294 \ub2e4\uc74c\uacfc \uac19\uc774 \uc815\uc758\ud569\ub2c8\ub2e4.</p> \\[ \\varepsilon = \\inf_{g_\\theta \\in \\mathcal{G}}\\| f - g_\\theta\\|. \\] <p>\uc989, \\(\\mathcal{G}\\)\ub77c\ub294 set \uc548\uc758 \ubaa8\ub4e0 candidate \\(g_\\theta\\) \uc911\uc5d0\uc11c \uac00\uc7a5 \uc88b\uc740 \uac83\uacfc \\(f\\) \uc0ac\uc774\uc758 \uac70\ub9ac\ub77c\ub294 \ub73b\uc785\ub2c8\ub2e4. \ub2e4\ub978 \ub9d0\ub85c \ud45c\ud604\ud558\uba74 approximation error\uac00 \uc791\uc744 \uc218\ub85d model class \\(\\mathcal{G}\\)\uc758 expressibility\uac00 \ud06c\ub2e4\ub294 \ub73b\uc774 \ub429\ub2c8\ub2e4. \ud568\uc218\uc758 \ud06c\uae30 \\(\\| \\cdot \\|\\)\ub97c \uc5b4\ub5bb\uac8c \uc815\uc758\ud558\ub290\ub0d0\uc5d0 \ub530\ub77c \\(\\varepsilon\\)\uc758 \uac12\uc774 \ub2ec\ub77c\uc9c8 \uc218 \uc788\uc9c0\ub9cc, \uc5ec\uae30\uc11c\ub294 \ub118\uc5b4\uac00\uaca0\uc2b5\ub2c8\ub2e4.</p>"},{"location":"1.%20Preliminaries/4.Errors/#estimation-error","title":"Estimation error","text":"<p>\\(f\\)\ub294 \ubd80\ubd84\uc801\uc73c\ub85c \uc8fc\uc5b4\uc838 \uc788\uc2b5\ub2c8\ub2e4. \ub530\ub77c\uc11c \uc6b0\ub9ac\uc5d0\uac8c \uc8fc\uc5b4\uc9c4 data\uc778 \uc9d1\ud569 \\(\\{(x_i, f(x_i)): i=0, \\dots, N\\}\\)\uc740 \uc720\ud55c\uc9d1\ud569\uc785\ub2c8\ub2e4. \ub370\uc774\ud130\uac00 \uc720\ud55c\ud568\uc5d0\uc11c \uc624\ub294 error\uac00 estimation error\uc785\ub2c8\ub2e4.</p> <p>\uc6d0\ub798 \uc6b0\ub9ac\uac00 minimize\ud558\uace0 \uc2f6\uc740 \ud568\uc218\ub97c</p> \\[     \\mathcal{L}(\\theta) = \\int (f(x) - g_\\theta(x))^2dx \\] <p>\ub77c\uace0 \ud558\uba74, \ub370\uc774\ud130\uc758 \uc720\ud55c\ud568\uc73c\ub85c \uc778\ud574 \ub300\uc2e0</p> \\[     L(\\theta) = \\sum_{i=0}^N(f(x_i) - g_\\theta(x_i))^2 \\] <p>\uc744 \ucd5c\uc18c\ud654\ud558\uac8c \ub429\ub2c8\ub2e4. \uc801\ubd84\uc744 sum\uc73c\ub85c \ud45c\ud604\ud560 \uc218\ubc16\uc5d0 \uc5c6\ub294 \uc0c1\ud669\uc5d0\uc11c estimation error\uac00 \ubc1c\uc0dd\ud55c\ub2e4\uace0 \ubcf4\uc2dc\uba74 \ub429\ub2c8\ub2e4.</p> <p>PINN\uc758 \uad00\uc810\uc73c\ub85c \ub3cc\uc544\uc640\uc11c, \uc6b0\ub9ac\uc758 \ubaa9\uc801\uc740 PDE\uc758 solution\uc744 \ucc3e\ub294 \uac83\uc785\ub2c8\ub2e4. \uc5b4\ub5bb\uac8c \ubcf4\uba74 explicit\ud55c \ub370\uc774\ud130\uac00 \uc5c6\uace0 physical law\ub9cc \uc8fc\uc5b4\uc9c4 \uc0c1\ud669\uc785\ub2c8\ub2e4. \uc774 \uacbd\uc6b0, PINN loss\ub97c \uc218\uce58\uc801\uc73c\ub85c \uacc4\uc0b0\ud558\uae30 \uc704\ud574 quadrature \ubc29\ubc95\uc73c\ub85c \uc801\ubd84\uc744 \uacc4\uc0b0\ud588\ub358 \uac83\uc744 \uae30\uc5b5\ud574\uc57c \ud569\ub2c8\ub2e4. Quadrature\ub294 continuous \uc801\ubd84\uc744 weighted sum\uc73c\ub85c \uadfc\uc0ac\ud558\ub294 \ubc29\ubc95\uc774\ubbc0\ub85c, \ud544\uc5f0\uc801\uc73c\ub85c error\uac00 \uc788\uc2b5\ub2c8\ub2e4. \ub530\ub77c\uc11c PINN\uc758 \uacbd\uc6b0 \uc5ed\uc2dc estimation error = qudrature error\ub77c\uace0 \uc0dd\uac01\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p>"},{"location":"1.%20Preliminaries/4.Errors/#optimization-error","title":"Optimization error","text":"<p>Loss \uac12\uc744 \uacc4\uc0b0\ud588\uc73c\uba74, \uc774\ub97c \uc904\uc774\ub294 \ubc29\ud5a5\uc73c\ub85c network parameter \\(\\theta\\)\ub97c \uc5c5\ub370\uc774\ud2b8 \ud574\uc57c \ud569\ub2c8\ub2e4. \ud558\uc9c0\ub9cc epoch\ub97c \uc544\ubb34\ub9ac \ud06c\uac8c \ud558\ub354\ub77c\ub3c4, round-off error\ub098 non-convexity\uc640 \uac19\uc740 \uc774\uc288\ub85c \uc778\ud574 \uc5bb\uc5b4\uc9c4 solution\uc774 \ucd5c\uc801\ud654 \ubb38\uc81c\uc758 global solution\uc774\ub77c\uace0 \ubcfc \uc218 \uc5c6\uc2b5\ub2c8\ub2e4. \uc5bb\uc5b4\uc9c4 \ucd5c\uc801\ud654 \ubb38\uc81c\uc758 solution\uacfc \uc774\ub860\uc0c1 solution \uc0ac\uc774\uc758 \ucc28\uc774\ub97c optimization error\ub77c\uace0 \ud569\ub2c8\ub2e4.</p>"},{"location":"1.%20Preliminaries/4.Errors/#implications","title":"Implications","text":"<ul> <li>Approximation error\ub294 network\uc758 size\ub97c \uc99d\uac00\uc2dc\ud0a4\uba74 \uc791\uc544\uc9d1\ub2c8\ub2e4 (Universal approximation theorem).</li> <li>Estimation error\ub294 sample points\uc758 \uac1c\uc218\ub97c \ub298\ub9ac\uba74 \uc791\uc544\uc9d1\ub2c8\ub2e4.</li> <li>Optimization error\ub294 epoch\ub97c \ub298\ub9ac\uace0 learning rate\uc744 \uc904\uc774\uba74 \uc791\uc544\uc9c8 \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4.</li> </ul> <p>\ud558\uc9c0\ub9cc \uc2e4\uc81c\ub85c\ub294 \uc774\ub807\uac8c \ub2e8\uc21c\ud558\uc9c0\ub9cc\uc740 \uc54a\uc2b5\ub2c8\ub2e4. Approximation error\ub97c \uc904\uc774\uae30 \uc704\ud574\uc11c network size\ub97c \ud0a4\uc6b0\uba74, \ucc3e\uc544\uc57c \ud558\ub294 parameter \uac1c\uc218\uac00 \ub9ce\uc544\uc9c0\uace0, \uc774\ub294 optimization\uc774 \uc5b4\ub824\uc6cc\uc9c8 \uc218\ub3c4 \uc788\uc74c\uc744 \uc774\uc57c\uae30 \ud569\ub2c8\ub2e4. Approximation error\ub294 \uc904\uc5c8\uc9c0\ub9cc optimization error\uac00 \uc99d\uac00\ud574\uc11c \uc804\uccb4\uc801\uc778 \uc5d0\ub7ec\ub294 \uc99d\uac00\ud560 \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4. \ub530\ub77c\uc11c \uc138 \uc5d0\ub7ec\uc758 \uc801\uc808\ud55c balance\ub97c \ucc3e\ub294 \uac83\uc740 PINN \ubfd0\ub9cc \uc544\ub2c8\ub77c, deep learning, \ub354 \ub098\uc544\uac00 \ub9ce\uc740 \uc885\ub958\uc758 numerical computation \ubb38\uc81c\uc5d0 \ub300\ud558\uc5ec \uc911\uc694\ud569\ub2c8\ub2e4.</p>"},{"location":"2.%20PINNs/","title":"Physics-informed neural networks","text":"<p>Phyics-informed neural networks (PINNs)\ub294 \ub370\uc774\ud130 \uae30\ubc18 \uba38\uc2e0\ub7ec\ub2dd \ubaa8\ub378\uc774 \uc54c\uace0\uc788\ub294 \ubb3c\ub9ac\ubc95\uce59 (\uc8fc\ub85c \ubbf8\ubd84\ubc29\uc815\uc2dd\uc73c\ub85c \uae30\uc220)\uc744 \ub530\ub974\ub3c4\ub85d \ud558\ub294 \ubc29\ubc95\uc785\ub2c8\ub2e4. \ubc29\ubc95\uc740 \uc544\uc8fc \uac04\ub2e8\ud569\ub2c8\ub2e4. \uba38\uc2e0\ub7ec\ub2dd \ubaa8\ub378\uc5d0 parameter\uac00 sparse\ud558\ub2e4\ub294 \ucd94\uac00\uc801\uc778 \uc81c\uc57d \uc870\uac74\uc744 \uac78\uae30 \uc704\ud574\uc11c\ub294 \uc8fc\ub85c Lasso penalty\ub97c \uace0\ub824\ud569\ub2c8\ub2e4<sup>1</sup>. \ube44\uc2b7\ud558\uac8c, \uba38\uc2e0\ub7ec\ub2dd \ubaa8\ub378\uc774 \ubbf8\ubd84\ubc29\uc815\uc2dd\uc744 \ub9cc\uc871\ud574\uc57c \ud55c\ub2e4\uba74, \ubbf8\ubd84\ubc29\uc815\uc2dd\uc744 \ub9cc\uc871\ud558\ub3c4\ub85d \ud558\ub294 penalty \ud568\uc218\ub97c \uace0\uc548\ud574\uc11c objective function\uc5d0 \ub354\ud574\uc8fc\uba74 \ub429\ub2c8\ub2e4. \uc774 penalty\ub97c Physics-informed loss \ud639\uc740 PINN loss\ub77c\uace0 \ubd80\ub985\ub2c8\ub2e4.</p> <p>\ud765\ubbf8\ub85c\uc6b4 \uc810\uc740 data\uac00 \uc5c6\uc744 \ub54c PINN loss\ub97c minimize \ud558\ub294 \ubaa8\ub378\uc740 \ubbf8\ubd84\ubc29\uc815\uc2dd\uc758 \ud574\uac00 \ub41c\ub2e4\ub294 \uac83\uc785\ub2c8\ub2e4. \ubcf8 \ucc45\uc5d0\uc11c\ub294 data\uac00 \uc5c6\ub294 \uacbd\uc6b0\ub97c \uac00\uc815\ud558\uace0 PINN loss\ub97c minimize\ud558\ub294 \ubc29\ubc95\uc5d0 \ub300\ud574\uc11c\ub9cc \ub2e4\ub8f9\ub2c8\ub2e4. \ub9cc\uc57d \ubd84\uc11d\ud558\uace0 \uc2f6\uc740 data\uac00 \uc788\ub294 \uacbd\uc6b0\uc5d0\ub294, \uc0ac\uc6a9\ud558\ub358 \ubaa8\ub378\uc5d0\ub2e4 PINN loss\ub97c \ucd94\uac00\ud558\uc5ec \ubd84\uc11d\ud568\uc73c\ub85c\uc368 model\uc774 \ubb3c\ub9ac\ubc95\uce59\uc744 \ub530\ub974\ub3c4\ub85d \uc720\ub3c4\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p>"},{"location":"2.%20PINNs/#physics-informed-loss","title":"Physics-informed loss","text":"<p>\ub2e4\uc74c\uacfc \uac19\uc740 \ud3b8\ubbf8\ubd84\ubc29\uc815\uc2dd\uc774 \uc788\uc2b5\ub2c8\ub2e4. $$     \\mathcal{D}[u] (x) = f(x), \\quad x \\in \\Omega, $$ $$     \\mathcal{B}[u] (x) = g(x), \\quad x \\in \\partial \\Omega. $$ \uc5ec\uae30\uc11c \\(u\\)\ub294 \ud3b8\ubbf8\ubd84\ubc29\uc815\uc2dd\uc758 \ud574, \\(\\mathcal{D}\\)\uc740 differential operator, \\(\\sigma\\)\ub294 surface measure, \uadf8\ub9ac\uace0 \\(\\mathcal{B}\\)\ub294 boundary condition\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.</p> <p>Evolutionary equation\uc758 \uacbd\uc6b0 \\(t\\)\ub97c \\(x\\)\uc5d0 \ud3ec\ud568\uc2dc\ucf1c \\(\\mathcal{B}\\)\uac00 initial condition\ub3c4 \ub098\ud0c0\ub0b4\uac8c \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p>"},{"location":"2.%20PINNs/#definition-physics-informed-loss","title":"Definition - Physics-informed loss","text":"<p>Physics-informed loss\ub97c \uc815\uc758\ud558\uaca0\uc2b5\ub2c8\ub2e4. Physics-informed loss \\(\\mathcal{L}_\\mathrm{PINN}(\\theta)\\)\ub294 \ub2e4\uc74c\uacfc \uac19\uc774 \uc815\uc758\ud55c\ub2e4. $$     \\mathcal{L}_\\mathrm{PINN}(\\theta) = \\int_\\Omega \\left( \\mathcal{D}[u_\\theta] (x) - f(x) \\right)^2 dx + \\lambda \\int_{\\partial\\Omega} \\left( \\mathcal{B}[u_\\theta] (x) - g(x) \\right)^2 d\\sigma(x). $$</p> <p>\uc774 \ub54c physics-informed neural networks\ub294 \ub2e4\uc74c\uc744 \ub9cc\uc871\ud558\ub294 network parameter \\(\\theta^\\star\\)\ub97c \ucc3e\ub294 \ubc29\ubc95\uc785\ub2c8\ub2e4. $$     \\theta^\\star = \\arg \\min_{\\theta} \\mathcal{L}_\\mathrm{PINN}(\\theta) $$</p> <p>\ub9cc\uc57d \\(\\mathcal{L}_\\mathrm{PINN}(\\theta^\\star) = 0\\) \uc774\ub77c\uba74 \\(u_{\\theta^\\star}\\)\ub294 almost everywhere \ubbf8\ubd84\ubc29\uc815\uc2dd\uc744 \ub9cc\uc871\ud558\uac8c \ub418\ubbc0\ub85c \ubbf8\ubd84\ubc29\uc815\uc2dd\uc758 \ud574\uac00 \ub429\ub2c8\ub2e4.</p> <p>\ud558\uc9c0\ub9cc, \\(\\mathcal{L}\\)\uc740 \uc801\ubd84\uc744 \ud1b5\ud574 \uc815\uc758\ub418\uc5b4 \uc788\uae30\uc5d0 \uac12\uc744 \uc815\ud655\ud558\uac8c \uad6c\ud558\ub294 \uac83\uc740 \uc27d\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4. \ub530\ub77c\uc11c quadrature rule\uc744 \ud1b5\ud574 \uc801\ubd84 \uac12\uc744 approximation\ud558\uac8c \ub429\ub2c8\ub2e4.</p> <p>Quadrature rule\uc740 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4. $$     \\int_a^b f(x) dx \\approx \\sum_{i=1}^N w_i f(x_i). $$</p> <p>PINN \ubd84\uc57c\uc5d0\uc11c \uac00\uc7a5 \ud754\ud558\uac8c \uc4f0\ub294 quadrature rule\uc740 Monte-Carlo \uc785\ub2c8\ub2e4. Collocation points \\(x_i\\)\ub97c random distribution\uc5d0\uc11c \ubf51\uace0, \uadf8 \uc810\ub4e4\uc5d0\uc11c integrand\ub97c evaluation\ud55c \uac12\ub4e4\uc5d0 \ud3c9\uade0\uc744 \ucde8\ud558\uba74 \ub429\ub2c8\ub2e4. \uc0ac\uc2e4 domain\uc758 \ud06c\uae30\ub9cc\ud07c\uc744 \ud3c9\uade0\ub0b8 \uac12\uc5d0 \uacf1\ud574\uc918\uc57c \ud558\uc9c0\ub9cc, \uc774\ub294 \ud604\uc7ac \ud06c\uac8c \uc911\uc694\ud558\uc9c0 \uc54a\uc73c\ubbc0\ub85c \ub118\uc5b4\uac00\uaca0\uc2b5\ub2c8\ub2e4.</p> <p>\uc815\ub9ac\ud558\uba74, (empirical) PINN loss\ub294 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4.</p> \\[ L(\\theta) = \\frac{1}{N_r}\\sum_{i=1}^{N_r} \\left( \\mathcal{D}[u_\\theta](x_{i,r}) - f(x_{i,r}) \\right)^2 + \\frac{\\lambda}{N_{b}}\\sum_{j=1}^{N_b} \\left( \\mathcal{B}[u_\\theta](x_{j,b}) - g(x_{j,b})\\right)^2. \\] <ol> <li> <p>Tibshirani, R. (1996) Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society Series B: Statistical Methodology, 58, 267\u2013288.\u00a0\u21a9</p> </li> </ol>"},{"location":"2.%20PINNs/Optimization/","title":"Optimization (Training)","text":"<p>Loss function\uc744 \uc815\uc758\ud588\uc73c\uba74, \uc774\ub97c \ucd5c\uc18c\ud55c\uc73c\ub85c \ud558\ub294 \\(\\theta^\\star\\)\ub97c \ucc3e\uc544\uc57c \ud569\ub2c8\ub2e4. \uc774 \uacfc\uc815\uc744 optimization, \ud639\uc740 training\uc774\ub77c\uace0 \ud569\ub2c8\ub2e4.</p>"},{"location":"2.%20PINNs/Optimization/#gradient-descent","title":"Gradient Descent","text":"<p>Deep learning\uc5d0\uc11c\ub294 \ub2e4\uc591\ud55c optimizer\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \ub300\ubd80\ubd84\uc774 gradient descent\uc758 \ubcc0\ud615\uc785\ub2c8\ub2e4. Gradient descent\ub294 \\(\\nabla_\\theta L(\\theta)\\)\uc758 \uac12\uc744 \uc774\uc6a9\ud574\uc11c \\(L\\)\uc744 \uc904\uc5ec\ub098\uac00\ub294 \ubc29\ubc95\uc785\ub2c8\ub2e4.</p>"},{"location":"2.%20PINNs/Optimization/#definition-gradient-descent","title":"Definition - Gradient Descent","text":"<p>Gradient Descent. Loss function \\(L: \\mathbb{R}^p \\rightarrow \\mathbb{R}\\)\uc774 \uc788\uc744 \ub54c, radient descent with step size \\(\\eta\\)\ub294 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4. $$     \\theta^{n+1} = \\theta^{n} - \\eta \\nabla_\\theta L(\\theta^n). $$</p> <p>\ud55c \ubc88\uc5d0 minimizer\ub97c \ucc3e\ub294 \uac83\uc774 \uc544\ub2c8\uace0, \uc5ec\ub7ec \ubc88 \ubc18\ubcf5\uc744 \ud1b5\ud574 minimizer\ub85c \ub2e4\uac00\uac00\ub294 sequence\ub97c \ub9cc\ub4e4\uc5b4 minimizer\ub97c \ucc3e\ub294 \ubc29\ubc95\uc785\ub2c8\ub2e4. \\(n\\)\ubc88\uc9f8 \ubc18\ubcf5\uc5d0\uc11c parameter\ub97c \\(\\theta^n\\)\ub77c\uace0 \ud560 \ub54c, parameter\ub97c \\(\\theta^{n+1}\\)\ub85c \uc5c5\ub370\uc774\ud2b8 \ud558\ub294 \ubc29\ubc95\uc911\uc758 \ud558\ub098\uc785\ub2c8\ub2e4. Gradient descent\ub97c \uc720\ub3c4\ud558\ub294 \ubc29\ubc95\uc740 \uac04\ub2e8\ud569\ub2c8\ub2e4. \uba3c\uc800 \\(\\theta^n\\)\uc774 \uc788\uc744 \ub54c, parameter\ub97c \uc5c5\ub370\uc774\ud2b8 \ud558\ub294 \uac83\uc740 \ubc29\ud5a5 \\(d\\), \ud06c\uae30 \\(\\eta\\) \ub450\uac00\uc9c0\uac00 \ud544\uc694\ud569\ub2c8\ub2e4. \uc5ec\uae30\uc11c \uc5c5\ub370\uc774\ud2b8 \ud06c\uae30\uac00 step size \\(\\eta\\)\uac00 \ub429\ub2c8\ub2e4. \ubc29\ud5a5\uc744 \ucc3e\ub294 \ubc29\ubc95 \uc911 \ud558\ub098\uac00 gradient descent\uc785\ub2c8\ub2e4. \uba3c\uc800 \\(L\\)\uc744 \\(\\theta\\)\uc5d0 \ub300\ud574 linearization \ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4. $$     L(\\theta + d) \\approx L(\\theta) + \\nabla_\\theta L(\\theta) \\cdot d. $$ \uc6b0\ub9ac\ub294 \uc88c\ubcc0\uc744 \\(d\\)\uc5d0 \ub300\ud574 minimization\ud558\uace0 \uc2f6\uc2b5\ub2c8\ub2e4. \ud558\uc9c0\ub9cc \uc774\ub294 \uc5b4\ub824\uc6b0\ubbc0\ub85c \ub300\uc2e0 \uc6b0\ubcc0\uc744 minimization \ud569\ub2c8\ub2e4. \uc6b0\ubcc0\uc740 linear\ud558\uae30 \ub54c\ubb38\uc5d0, \uc6b0\ubcc0\uc774 \uc904\uc5b4\ub4dc\ub294 \ubc29\ud5a5\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4. 1\ucc28 \ud568\uc218 \\(y = ax + b\\)\ub97c \uc0dd\uac01\ud574 \ubcf4\uba74, \\(a\\)\uac00 \uc591\uc218\uc778 \uacbd\uc6b0 \\(x\\)\uac00 \uac10\uc18c\ud558\uba74 \ud568\uc218\uac00 \uac10\uc18c\ud569\ub2c8\ub2e4. \uc989 \uae30\uc6b8\uae30\uc640 \ubc18\ub300 \ubc29\ud5a5\uc73c\ub85c \uc6c0\uc9c1\uc5ec\uc57c \ud569\ub2c8\ub2e4. \uae30\uc6b8\uae30\uac00 \\(\\nabla_\\theta L(\\theta)\\)\ub85c \uc8fc\uc5b4\uc84c\uc73c\ubbc0\ub85c \uadf8 \ubc18\ub300\uc778 \\(d = -\\nabla_\\theta L(\\theta)\\)\ub97c \uc120\ud0dd\ud558\uba74, $$     L(\\theta) + \\nabla_\\theta L(\\theta) \\cdot d = L(\\theta) - | \\nabla_\\theta L(\\theta) |_2^2 &lt; L(\\theta) $$ \uac00 \ub418\uc5b4 \\(L\\)\uc774 \uc904\uc5b4\ub4e6\uc744 \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ud558\uc9c0\ub9cc linearization\uc740 \\(d\\)\uac00 \uc791\uc744 \ub54c\ub9cc \uc131\ub9bd\ud569\ub2c8\ub2e4. \ub530\ub77c\uc11c \\(\\eta\\)\ub97c \uc791\uac8c \ud560 \ud544\uc694\uac00 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>Gradient descent\ub294 computational cost\uac00 \uc800\ub834\ud569\ub2c8\ub2e4. Reverse mode AD\ub97c \uc774\uc6a9\ud574\uc11c \\(\\nabla_\\theta L(\\theta)\\)\ub97c \ub9e4\uc6b0 \uc800\ub834\ud558\uac8c \uacc4\uc0b0\ud560 \uc218 \uc788\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4. \ud558\uc9c0\ub9cc \uc218\ub834 \uc18d\ub3c4\uac00 \ub290\ub9ac\ub2e4\ub294 \ub2e8\uc810\uc774 \uc788\uc2b5\ub2c8\ub2e4. Loss function\uc774 flat\ud55c \uacf3\uc5d0\uc11c\ub294 linearization\uc774 \ube44\uad50\uc801 \uc815\ud655\ud558\ubbc0\ub85c \\(\\eta\\)\ub97c \ud06c\uac8c, \ubc18\ub300\ub85c sharp\ud55c \uacf3\uc5d0\uc11c\ub294 \\(\\eta\\)\ub97c \uc791\uac8c \uac00\uc838\uac00\uc57c \ud569\ub2c8\ub2e4. \uc774\ub7ec\ud55c \uc815\ubcf4\ub97c curvature information\uc774\ub77c\uace0 \ud558\uace0, \\(L\\)\uc744 \ub450\ubc88 \ubbf8\ubd84\ud574\uc11c \uc5bb\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4. Gradient descent\ub294 curvature information\uc744 \uace0\ub824\ud558\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.</p>"},{"location":"2.%20PINNs/Optimization/#newtons-method","title":"Newton's method","text":"<p>\\(L(\\theta + d)\\)\ub97c linearization \ud558\uc9c0 \ub9d0\uace0, quadratic approximation\uc744 \ud55c \ud6c4 minimizing \\(d\\)\ub97c \ucc3e\uc73c\uba74 Newton's method\uac00 \ub429\ub2c8\ub2e4.</p>"},{"location":"2.%20PINNs/Optimization/#definition-newtons-method","title":"Definition - Newton's method","text":"<p>$$     \\theta^{n+1} = - \\eta H(\\theta^n)^\\dagger \\nabla_\\theta(\\theta^n). $$ \uc5ec\uae30\uc11c \\(H\\)\ub294 Hessian matrix \\([H(\\theta)]_{ij} = \\frac{\\partial^2 L(\\theta)}{\\partial \\theta_i \\partial \\theta_j}\\) \uc785\ub2c8\ub2e4. \\(A^\\dagger\\)\ub294 Moore-Penrose pseudo inverse \uc785\ub2c8\ub2e4. \uadf8\ub0e5 matrix inverse\ub77c\uace0 \uc0dd\uac01\ud558\uc154\ub3c4 \ubb34\ubc29\ud569\ub2c8\ub2e4.</p> <p>Newton's method\ub97c \uc720\ub3c4\ud574 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4. $$     L(\\theta + d) \\approx L(\\theta) + \\nabla_\\theta L(\\theta) \\cdot d + \\frac{1}{2}d^T H(\\theta) d. $$ \uc704 \uc2dd\uc740 Taylor \uc815\ub9ac\ub97c \uc0ac\uc6a9\ud558\uba74 \uc27d\uac8c \uc720\ub3c4\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc544\uae4c \ud588\ub358 \uac83\ucc98\ub7fc, \uc88c\ubcc0\uc744 minimization \ud558\ub294 \ub300\uc2e0 \uc6b0\ubcc0\uc5d0\uc11c \uc815\ubcf4\ub97c \ubf51\uc544\ub0b4\uc5b4 \\(L\\)\uc744 \uc904\uc77c \uc218 \uc788\ub294 direction \\(d\\)\ub97c \ucc3e\ub294\uac8c \ubaa9\ud45c\uc785\ub2c8\ub2e4. \uc774\uc804\uacfc \ub2e4\ub974\uac8c, \uc6b0\ubcc0\uc774 quadratic function\uc774\uace0 \\(H(\\theta)\\)\uac00 positive semidefinite matrix\uc774\ubbc0\ub85c unique minimizer\uac00 \uc874\uc7ac\ud569\ub2c8\ub2e4. 2\ucc28 \ud568\uc218 \\(q\\)\ub294 \\(q'(x) = 0\\)\uc778 \uc810 \\(x\\)\uc5d0\uc11c extreme value\ub97c \uac00\uc9d1\ub2c8\ub2e4. \ub9c8\ucc2c\uac00\uc9c0\ub85c, $$     \\nabla_d \\left( L(\\theta) + \\nabla_\\theta L(\\theta) \\cdot d + \\frac{1}{2}d^T H(\\theta) d \\right) = 0 $$ \uc778 \uc810 \\(d\\)\uc5d0\uc11c \ucd5c\uc19f\uac12\uc744 \uac00\uc9d1\ub2c8\ub2e4. \uacc4\uc0b0\uc744 \ud574 \ubcf4\uba74, \uc774\ub294 \\(d = - H(\\theta)^\\dagger \\nabla_\\theta L(\\theta)\\)\uac00 \ub429\ub2c8\ub2e4. Newton's method \uc5ed\uc2dc \\(d\\)\uac00 \uc791\uc744 \ub54c quadratic approximation\uc774 \uc758\ubbf8\uac00 \uc788\uc2b5\ub2c8\ub2e4. \ub530\ub77c\uc11c \\(\\eta\\) \uac12\uc744 \uc801\ub2f9\ud788 \uc870\uc808\ud574 \uc904 \ud544\uc694\uac00 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>Newton's method\ub294 \uc218\ub834 \uc18d\ub3c4\uac00 \ud6cc\ub96d\ud569\ub2c8\ub2e4. Minimizer\ub97c scientific notation\uc73c\ub85c \ub098\ud0c0\ub0c8\uc744 \ub54c, \uc815\ud655\ud55c decimal points\uac00 iteration \ud55c \ubc88 \uc99d\uac00\ud560 \ub54c \ub9c8\ub2e4 \ub450\ubc30\uac00 \ub429\ub2c8\ub2e4 (quadratic convergence.) \ud558\uc9c0\ub9cc \ub9e4\uc6b0 \ud070 \ub2e8\uc810\uc774 \uc788\uc2b5\ub2c8\ub2e4. \\(H(\\theta)^\\dagger \\nabla_\\theta(\\theta)\\)\ub97c \uacc4\uc0b0\ud558\ub294 \uac83\uc774 \uc27d\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4. $$     H(\\theta) d = \\nabla_\\theta(\\theta) $$ \ub97c linear system\uc73c\ub85c \ub9cc\ub4e4\uc5b4\uc11c \ud480\uba74 \ub418\uc9c0\ub9cc, direct method\ub85c \ud480 \uacbd\uc6b0 \\(\\mathcal{O}(p^2)\\) \ub9cc\ud07c\uc758 \uba54\ubaa8\ub9ac\uac00 \ud544\uc694\ud569\ub2c8\ub2e4. Neural network\uc758 parameter \uac1c\uc218\uac00 \ubcf4\ud1b5 \ud06c\ub2e4\ub294 \uac83\uc744 \uc0dd\uac01\ud560 \ub54c, \uc774 \uba54\ubaa8\ub9ac \ube44\uc6a9\uc740 \uaf64\ub098 \ud07d\ub2c8\ub2e4. Matrix-free method (e.g. conjugate gradient)\ub294 Hessian matrix\ub97c explicit\ud558\uac8c \ub9cc\ub4e4 \ud544\uc694\ub294 \uc5c6\uc9c0\ub9cc, condition number\uac00 \ud06c\uba74 \ubd80\uc815\ud655\ud569\ub2c8\ub2e4. \uadf8\ub9ac\uace0 matrix-free \ubc29\ubc95 \uc5ed\uc2dc \\(\\mathcal{O}(p^3)\\) operation cost\uac00 \ud544\uc694\ud569\ub2c8\ub2e4.</p>"},{"location":"2.%20PINNs/Optimization/#other-variants","title":"Other Variants","text":"<p>\ub2e4\uc591\ud55c \ubc29\ubc95\uc744 \ud1b5\ud574 {eq}<code>PINN-Loss</code>\ub97c minimize \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ubcf4\ud1b5 adam optimizer\ub97c \ub9ce\uc774 \uc0ac\uc6a9\ud569\ub2c8\ub2e4<sup>1</sup>. \ub2e4\ub978 \uc120\ud0dd\uc73c\ub85c\ub294 L-BFGS\uac00 \uc788\uc2b5\ub2c8\ub2e4<sup>2</sup>. Adam\uc740 \uc798 \ub3d9\uc791\ud558\uc9c0\ub9cc L-BFGS\ub294 \uc798 \ub3d9\uc791\ud558\uc9c0 \uc54a\ub294 \uacbd\uc6b0\uac00 \uc788\uace0, \uadf8 \ubc18\ub300\uc758 \uacbd\uc6b0\ub3c4 \uc788\uace0, \ub458 \ub2e4 \uc798 \ub3d9\uc791\ud558\uc9c0 \uc54a\ub294 \uacbd\uc6b0\uac00 \uc788\uc2b5\ub2c8\ub2e4. \ub530\ub77c\uc11c \ub2e4\uc591\ud55c optimizer\uc758 \ud2b9\uc131\uc744 \ubbf8\ub9ac \uc54c\uc544\ub450\uace0 \uc0c1\ud669\uc5d0 \ub9de\uac8c \uc120\ud0dd\ud560 \ud544\uc694\uac00 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>Adam\uc740 stochastic \uc138\ud305\uc5d0\uc11c \uc798 \ub3d9\uc791\ud569\ub2c8\ub2e4. Adaptive weight\uc774\ub098 stochastic collocation points sampling \uac19\uc740 \uc54c\uace0\ub9ac\uc998\uc744 \uc801\uc6a9\ud560 \ub54c\ub294 \uc8fc\ub85c adam\uc744 \uba3c\uc800 \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \uadf8 \ub2e4\uc74c\uc5d0 \uc5b4\ub290\uc815\ub3c4 PINN loss\uac00 \uc904\uc5b4\ub4e4\uc5c8\ub2e4\uace0 \uc0dd\uac01\ub418\uba74, weight\uacfc collocation points\ub97c \ubaa8\ub450 \uace0\uc815\ud558\uace0 L-BFGS\ub85c \ucd94\uac00\uc801\uc778 optimization\uc744 \uac00\uc838\uac00\ub294 \uacbd\uc6b0\ub3c4 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>L-BFGS\ub294 loss function\uc758 curvature \uc815\ubcf4\ub97c \uc0ac\uc6a9\ud558\ubbc0\ub85c, \uc77c\ubc18\uc801\uc73c\ub85c adam\ubcf4\ub2e4 \ud37c\ud3ec\uba3c\uc2a4\uac00 \uc88b\uc2b5\ub2c8\ub2e4. \ud558\uc9c0\ub9cc stochastic \uc138\ud305\uc5d0\uc11c\ub294 \uc798 \ub3d9\uc791\ud558\uc9c0 \uc54a\uc73c\uba70, \uacfc\uac70\uc5d0 \uacc4\uc0b0\ud588\ub358 gradient \uc815\ubcf4\ub97c \uc800\uc7a5\ud574\ub46c\uc57c \ud558\ubbc0\ub85c adam\ubcf4\ub2e4 \ub354 \ub9ce\uc740 \uba54\ubaa8\ub9ac\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4.</p> <p>Parameter space\uc5d0 Riemannian metric\uc73c\ub85c Fisher matrix\ub97c \uc8fc\uace0 \uc774 manifold \uc704\uc5d0\uc11c Newton's method\ub97c \ub530\ub77c PINN loss\ub97c \uc904\uc774\ub294 \ubc29\ubc95\ub3c4 \uc788\uc2b5\ub2c8\ub2e4<sup>3</sup>. \uadf8\ub7ec\ub098 \uc774 \ubc29\ubc95\uc740 Hessian matrix \\(H\\)\uc5d0 \ub300\ud558\uc5ec \\(Hx=b\\)\ub97c \ud480\uc5b4\uc57c \ud558\uc9c0\ub9cc, \\(H\\)\uc758 condition number\uac00 \ub9ce\uc740 \uacbd\uc6b0 \ud06c\uae30 \ub54c\ubb38\uc5d0 \uc2e4\uc6a9\uc801\uc774\ub77c\uace0 \ubcf4\uae30 \uc5b4\ub835\uc2b5\ub2c8\ub2e4.</p> <p>Neural network\ub97c \uc5ec\ub7ec\uac1c \ub3c4\uc785\ud558\uc5ec \uc21c\ucc28\uc801\uc73c\ub85c PINN loss\ub97c \uc904\uc5ec\uc11c \uc815\ud655\uc131\uc744 machine epsilon \\(10^{-16}\\) \uc218\uc900\uc73c\ub85c \ub2ec\uc131\ud560 \uc218 \uc788\ub294 \ubc29\ubc95\ub3c4 \uc788\uc2b5\ub2c8\ub2e4<sup>4</sup>. Adam\uacfc L-BFGS\ub9cc \uc0ac\uc6a9\ud588\uae30 \ub54c\ubb38\uc5d0 \uc2e4\uc6a9\uc801\uc774\uc9c0\ub9cc, \ubbf8\ubd84\ubc29\uc815\uc2dd\uc758 order\uac00 \ub192\uc740 \uacbd\uc6b0\ub098 nonlinearity\uac00 \uc788\ub294 \uacbd\uc6b0\uc5d0\ub294 \uc544\uc9c1 \ub354 \uc5f0\uad6c\uac00 \ud544\uc694\ud569\ub2c8\ub2e4.</p> <ol> <li> <p>Kingma, D.P. &amp; Ba, J. (2014) Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.\u00a0\u21a9</p> </li> <li> <p>Liu, D.C. &amp; Nocedal, J. (1989) On the limited memory BFGS method for large scale optimization. Mathematical programming, 45, 503\u2013528.\u00a0\u21a9</p> </li> <li> <p>M\u00fcller, J. &amp; Zeinhofer, M. (2023) Achieving high accuracy with PINNs via energy natural gradient descent. International conference on machine learning. PMLR, pp. 25471\u201325485.\u00a0\u21a9</p> </li> <li> <p>Wang, Y. &amp; Lai, C.-Y. (2024) Multi-stage neural networks: Function approximator of machine precision. Journal of Computational Physics, 504, 112865.\u00a0\u21a9</p> </li> </ol>"},{"location":"3.%20Training/GradientFlow/","title":"Gradient Flow Pathologies","text":""},{"location":"3.%20Training/GradientFlow/#stiff-equations","title":"Stiff Equations","text":"<p>Time dependent \ubbf8\ubd84 \ubc29\uc815\uc2dd\uc740 \uc77c\ubc18\uc801\uc73c\ub85c \ub2e4\uc74c \ud615\ud0dc\ub97c \uac00\uc9d1\ub2c8\ub2e4.</p> \\[     \\frac{d}{dt}y(t) = f(t, y(t)), \\quad y(0) = y_0. \\] <p>\uc774\uc640 \uac19\uc740 \ubc29\uc815\uc2dd\uc740 \uba3c\uc800 time step\uc744 \uc798\uac8c \ucabc\uac20 \ub2e4\uc74c, time grid \uc704\uc5d0\uc11c \ubc29\uc815\uc2dd\uc744 discretize \ud558\uace0, \uc801\ub2f9\ud55c update rule\uc744 \ub530\ub77c\uc11c discretized solution\uc744 \uad6c\ud558\uac8c \ub429\ub2c8\ub2e4. \uc218 \uc5c6\uc774 \ub9ce\uc740 discretization \ubc29\ubc95\uc774 \uc788\uc9c0\ub9cc, \uadf8 \uc911\uc5d0\uc11c \uac00\uc7a5 \uac04\ub2e8\ud55c forward Euler \ubc29\ubc95\ub9cc\uc744 \uc0b4\ud3b4\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.</p> <p>$$     \\frac{y^{n+1} - y^n}{\\Delta t} = f(t^n, y^n). $$ \uc6b0\ubcc0\uc774 \\(y^{n+1}\\)\uc5d0 depend\ud558\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4. \uc774\ub7f0 \ubc29\ubc95\uc744 explicit method\ub77c\uace0 \ud569\ub2c8\ub2e4. \ub9cc\uc57d \uc6b0\ubcc0\uc758 \\(y^n\\)\uc744 \\(y^{n+1}\\)\ub85c \ubc14\uafbc\ub2e4\uba74, \uc774\ub294 implicit methods \uc911\uc758 \ud558\ub098\uc778 backward Euler \ubc29\ubc95\uc774 \ub429\ub2c8\ub2e4.</p> <p>\ubbf8\ubd84\ubc29\uc815\uc2dd\uc774 stiff\ud558\ub2e4\ub294 \uac83\uc758 \uc758\ubbf8\ub294, explicit method\uac00 \uc798 \uc791\ub3d9\ud558\uc9c0 \uc54a\ub294\ub2e4\ub294 \ub73b\uc785\ub2c8\ub2e4. \ub2e4\uc74c \uc608\ub97c \ubcf4\uaca0\uc2b5\ub2c8\ub2e4.</p>"},{"location":"3.%20Training/GradientFlow/#example-stiff-ordinary-differential-equation","title":"Example - Stiff Ordinary Differential Equation","text":"\\[     y'(t) = -a y(t), \\quad a &gt; 0, \\quad t \\ge 0, \\quad y(0) = y_0. \\] <p>\uc774 \ubc29\uc815\uc2dd\uc758 \ud574\ub294 \\(y(t) = y_0 e^{-at}\\)\uc785\ub2c8\ub2e4. \ud574\ub97c \uc774\ubbf8 \uc54c\uace0 \uc788\uc9c0\ub9cc, stiffness\uac00 \ubb34\uc5c7\uc778\uc9c0 \uc124\uba85\ud558\uae30 \uc704\ud574 \uc774\ub97c forward Euler \ubc29\ubc95\uc73c\ub85c \ud47c\ub2e4\uace0 \uc0dd\uac01\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4. Forward Euler update rule\uc5d0 \ud574\ub2f9\ub418\ub294 \uc810\ud654\uc2dd (recurrence relation)\uc740 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4.</p> \\[     y^{n+1} = (1 - a \\Delta t)y^n. \\] <p>\ub9cc\uc57d \\(|1 - a \\Delta t| \\ge 1\\)\uc774\ub77c\uba74, \uc218\uc5f4 {math}<code>\\{ y^n \\}</code>\uc740 \ubc1c\uc0b0\ud569\ub2c8\ub2e4. \ub530\ub77c\uc11c \\(|1 - a \\Delta t | &lt; 1\\)\uc774\ub780 \uc870\uac74\uc774 \ud544\uc694\ud569\ub2c8\ub2e4. \uc815\ub9ac\ud558\uba74</p> <p>$$     \\Delta t &lt; \\frac{2}{a} $$ \uc785\ub2c8\ub2e4. \ub530\ub77c\uc11c \\(a&gt;0\\)\uac00 \ucee4\uc9c8\uc218\ub85d, \ub354 \uc791\uc740 \\(\\Delta t\\)\uac00 \ud544\uc694\ud558\uac8c \ub429\ub2c8\ub2e4. \ubc18\uba74 backward Euler \ubc29\ubc95\uc740 \uc774\ub7ec\ud55c \uc81c\uc57d\uc870\uac74\uc774 \uc5c6\uc2b5\ub2c8\ub2e4.</p> <p>\uc774\ubc88\uc5d0\ub294 \\(y: [0, \\infty) \\rightarrow \\mathbb{R}^p\\)\uc778 \uacbd\uc6b0\ub97c \uc0dd\uac01\ud574 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4.</p>"},{"location":"3.%20Training/GradientFlow/#example-stiff-system-of-ordinary-differential-equations","title":"Example - Stiff System of Ordinary Differential Equations","text":"<p>$$     y'(t) = - A y(t), $$ \\(A\\)\ub294 symmetric positive definite matrix\ub77c\uace0 \uac00\uc815\ud558\uaca0\uc2b5\ub2c8\ub2e4. (\\(a&gt;0\\)\uc758 generalization.)</p> <p>Eigenvalue decomposition\uc744 \ud1b5\ud574 \\(A = Q \\Lambda Q^*\\)\ub85c \uc4f8 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \\(A\\)\uac00 symmetric \ud558\uae30 \ub54c\ubb38\uc5d0 \\(Q\\)\uac00 orthogonal matrix\uac00 \ub418\uace0 \\(Q^* = Q^{-1}\\), \uadf8\ub9ac\uace0 positive definite\uc774\uae30 \ub54c\ubb38\uc5d0 \ubaa8\ub4e0 eigenvalue\uac00 \uc591\uc218\uc785\ub2c8\ub2e4. \ub530\ub77c\uc11c {eq}<code>ode-p</code>\ub294 \ub2e4\uc74c\uacfc \uac19\uc774 \ub2e4\uc2dc \uc4f8 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p> \\[     \\frac{d}{dt}Q^* y(t) = -\\Lambda Q^* y(t). \\] <p>\uc5ec\uae30\uc11c \\(z(t) = Q^* y(t)\\)\ub77c\uace0 \ud558\uba74 \uc704 \ubbf8\ubd84\ubc29\uc815\uc2dd\uc740 decouple \ub41c \\(p\\)\uac1c\uc758 \ubbf8\ubd84\ubc29\uc815\uc2dd</p> <p>$$     z_i'(t) = -\\lambda_i z_i(t) $$ \uac00 \ub429\ub2c8\ub2e4.</p> <p>\uc5ec\uae30\uc5d0\uc11c \ubcf4\uc558\ub4ef\uc774, forward Euler \ubc29\ubc95\uc740 \\(\\Delta t &lt; 2 / \\max_i \\{\\lambda_i\\}\\)\ub77c\ub294 \uc870\uac74\uc774 \ud544\uc694\ud569\ub2c8\ub2e4.</p>"},{"location":"3.%20Training/GradientFlow/#example-stiff-partial-differential-equation","title":"Example - Stiff Partial Differential Equation","text":"<p>Heat equation\uc740 stiff PDE\uc911 \ud558\ub098\uc785\ub2c8\ub2e4.</p> \\[     \\partial_t u = \\partial_x^2 u. \\] <p>\uba3c\uc800 Fourier transform\uc744 {eq}<code>heat</code>\uc758 \uc591\ubcc0\uc5d0 \ucde8\ud558\uba74 \ub2e4\uc74c\uc744 \uc5bb\uc2b5\ub2c8\ub2e4.</p> \\[ \\partial_t \\hat{u}(\\xi) = (i\\xi)^2 \\hat{u}(\\xi). \\] <p>\ub530\ub77c\uc11c forward Euler \ubc29\ubc95\uc740 \\(\\Delta t &lt; 2 / \\xi^2\\)\ub77c\ub294 \uc870\uac74\uc774 \ud544\uc694\ud569\ub2c8\ub2e4. \\(x\\) \ubc29\ud5a5\uc73c\ub85c grid\ub97c \uc798\uac8c \uc790\ub97c \uc218\ub85d \\(\\xi\\)\uc758 \uac12\uc740 \ucee4\uc9d1\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4 \\(x\\) \ubc29\ud5a5\uc73c\ub85c \\(N\\)\uac1c\uc758 grid point\ub97c \ub3c4\uc785\ud588\ub2e4\uba74, \\(\\max \\xi = N/2\\)\uac00 \ub429\ub2c8\ub2e4. \ub530\ub77c\uc11c \\(t\\) \ubc29\ud5a5\uc740 \\(\\Delta t = O(N^{-2})\\)\ub97c \ub9cc\uc871\ud574\uc57c \ud569\ub2c8\ub2e4.</p>"},{"location":"3.%20Training/GradientFlow/#gradient-flow","title":"Gradient Flow","text":"<p>PINN training\uc774 \uc2e4\ud328\ud558\ub294 \uc774\uc720\ub97c gradient flow\uc758 eigenvalue bias\ub85c \uc124\uba85\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4<sup>1</sup>. \uac04\ub2e8\ud55c \uc544\uc774\ub514\uc5b4\uc785\ub2c8\ub2e4. Adam\uacfc \uac19\uc740 optimizer\ub294 gradient descent \uae30\ubc18 \ubc29\ubc95\uc785\ub2c8\ub2e4. Gradient descent\ub294 gradient flow\ub97c forward Euler \ubc29\ubc95\uc73c\ub85c time discretization \ud558\uba74 \uc5bb\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub530\ub77c\uc11c gradient flow\ub97c \ubd84\uc11d\ud558\uba74, gradient descent\uc5d0 \ub300\ud55c \uc815\ubcf4\ub97c \uc5bb\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p>"},{"location":"3.%20Training/GradientFlow/#definition-gradient-flow","title":"Definition - Gradient Flow","text":"<p>Model parameter \\(\\theta\\)\uc5d0 \ub300\ud55c gradient flow\ub294 \ub2e4\uc74c\uacfc \uac19\uc774 \uc815\uc758\ud569\ub2c8\ub2e4.</p> \\[ \\frac{d\\theta(t)}{dt} = - \\nabla_\\theta L_\\mathrm{PINN}(\\theta(t)). \\] <p>\uc5ec\uae30\uc640 \ube44\uc2b7\ud558\uac8c \uc0dd\uacbc\uc9c0\ub9cc, \uc6b0\ubcc0\uc774 \\(\\theta(t)\\)\uc5d0 \ub300\ud574\uc11c linear\ud558\uc9c0 \uc54a\ub2e4\ub294 \ucc28\uc774\uc810\uc774 \uc788\uc2b5\ub2c8\ub2e4. \ub530\ub77c\uc11c linearization\uc744 \ud558\uace0 \ub098\uc11c \uc5ec\uae30\uc640 \ube44\uc2b7\ud55c analysis\ub97c \ud558\uba74 insight\ub97c \uc5bb\uc744 \uc218 \uc788\uc744 \uc9c0\ub3c4 \ubaa8\ub985\ub2c8\ub2e4.</p> <p>\uba3c\uc800 linearization \uacfc\uc815\uc744 \uc124\uba85\ud558\uaca0\uc2b5\ub2c8\ub2e4. \ud568\uc218 \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\)\uac00 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ub54c linearization\uc774\ub780, \uace0\uc815\ub41c \uc810 \\(x \\in \\mathbb{R}^n\\) \uadfc\ucc98\uc5d0\uc11c \\(f\\)\ub97c \uac00\uc7a5 \"\ube44\uc2b7\"\ud55c linear transformation \\(A_x \\in \\mathbb{R}^{m \\times n}\\)\ub97c \ucc3e\ub294 \uac83\uc744 \ub9d0\ud569\ub2c8\ub2e4. \uc218\ud559\uc801\uc73c\ub85c \ud45c\ud604\ud558\uba74,</p> <p>$$ f(x + h) = f(x) + A_x h + o(|h|) $$ \uc785\ub2c8\ub2e4. \uc5ec\uae30\uc11c \\(o(|h|)\\)\ub294 \\(\\lim_{|h|\\rightarrow 0} o(|h|) / |h| = 0\\)\ub97c \uc758\ubbf8\ud569\ub2c8\ub2e4. \\(|h|\\)\ubcf4\ub2e4 \ube60\ub974\uac8c \\(0\\)\uc73c\ub85c \uac04\ub2e4\ub294 \ub73b\uc785\ub2c8\ub2e4. \uc774 \ud589\ub82c, \ud639\uc740 linear transformation \\(A_x\\)\ub97c \\(f\\)\uc758 \\(x\\)\uc5d0\uc11c\uc758 \ubbf8\ubd84\uc774\ub77c\uace0 \ud558\uace0, Fr\u00e9chet derivative\ub77c\uace0\ub3c4 \ubd80\ub985\ub2c8\ub2e4.</p> <p>\uc790\uc138\ud788 \ubcf4\uba74, \\(|h|\\)\uac00 \\(0\\)\uc5d0 \uac00\uae4c\uc6b0\uba74 \\(f(x+h) = f(x) + A_x h\\)\ub85c \uc4f8 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \\(f\\)\uc758 \\(x\\) \uadfc\ubc29\uc5d0\uc11c\uc758 \uc6c0\uc9c1\uc784\uc774 \\(h \\mapsto A_x h\\)\ub77c\ub294 linear transformation\uc73c\ub85c approximation \ub41c\ub2e4\ub294 \ub73b\uc785\ub2c8\ub2e4. \ub9cc\uc57d \\(f\\)\uac00 \ubbf8\ubd84\uac00\ub2a5\ud558\ub2e4\uba74, \\(A_x = J_f(x)\\) \uc989 \\(f\\)\uc758 Jacobian\uc774 \ub429\ub2c8\ub2e4.</p> <p>\ub2e4\uc2dc Gradient Flow\ub85c \ub3cc\uc544\uc640 \uc6b0\ubcc0\uc744 \\(\\theta(t)\\)\uc5d0 \ub300\ud574 linearization \ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4. \\(\\nabla_\\theta L : \\mathbb{R}^p \\rightarrow \\mathbb{R}^p\\)\ub97c linearization\ud558\uae30 \uc704\ud574\uc11c\ub294</p> <p>$$ \\frac{\\partial \\left( \\nabla_\\theta L\\right)_i}{\\partial \\theta_j} = \\frac{\\partial^2 L}{\\partial \\theta_i \\partial \\theta_j}, $$ \uc989 \\(L\\)\uc758 Hessian matrix\uac00 \ud544\uc694\ud569\ub2c8\ub2e4. Linearization\uc744 \uc704\ud574\uc11c, \\(t_0 \\le t\\)\uc774\uace0 \\(t\\)\uc640 \uac00\uae4c\uc6b4 \\(t_0\\)\ub97c \ud558\ub098 \uc7a1\uaca0\uc2b5\ub2c8\ub2e4. \uadf8\ub7ec\uba74</p> <p>$$ \\frac{d \\theta(t)}{dt} =  - \\left( \\nabla_\\theta L(\\theta(t_0)) + H(\\theta(t_0))\\left(\\theta(t) - \\theta(t_0)\\right) \\right) + o(|\\theta(t) - \\theta(t_0)|) $$ \uc744 \uc5bb\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub2e4\uc2dc \uc815\ub9ac\ud558\uba74</p> <p>$$ \\frac{d \\theta(t)}{dt} = - H(\\theta(t_0))\\theta(t) - \\left( \\nabla_\\theta L(\\theta(t_0)) - H(\\theta(t_0)) \\theta(t_0)\\right) + o(|\\theta(t) - \\theta(t_0)|) $$ (linearized-gradient-flow) \uac00 \ub429\ub2c8\ub2e4. \\(o(|\\theta(t) - \\theta(t_0)|)\\) \uac12\uc740 \uc791\uc73c\ubbc0\ub85c \ubb34\uc2dc\ud560 \uc218 \uc788\ub2e4\uace0 \uac00\uc815\ud558\uaca0\uc2b5\ub2c8\ub2e4. \ub530\ub77c\uc11c Gradient Descent\ub97c \ud560 \ub54c\ub294 {eq}<code>linearized-gradient-flow</code>\ub97c forward Euler \ubc29\ubc95\uc73c\ub85c discretization\ud558\uac8c \ub429\ub2c8\ub2e4. \uc5ec\uae30\uc758 analysis\ub97c \ud1b5\ud574 \ubcf4\uba74, \\(H(\\theta(t_0))\\)\uc758 eigenvalues \\(\\lambda_i\\)\uc5d0 \ub300\ud574 \\(\\Delta t &lt; 2 / \\lambda_\\mathrm{max}\\)\ub97c \ub9cc\uc871\ud574\uc57c \ud569\ub2c8\ub2e4.</p> <p>\uc5ec\uae30\uc11c optimization\uc774 \uc798 \ub418\uc9c0 \uc54a\ub294 \uc774\uc720\ub97c \ucc3e\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4. Hessian\uc758 spectrum\uc774 \ub113\uc73c\uba74, \ub2e4\uc2dc \ub9d0\ud574\uc11c</p> <p>$$ |\\frac{\\lambda_\\mathrm{max}}{\\lambda_\\mathrm{min}}| \\gg 1 $$ \uc774\uba74, eigenvalue\uac00 \ud070 \ubc29\ud5a5\uc73c\ub85c\ub294 \uc791\uc740 learning rate\uc774 \ud544\uc694\ud558\uc9c0\ub9cc eigenvalue\uac00 \uc791\uc740 \ubc29\ud5a5\uc73c\ub85c\ub294 \ud070 learning rate\uc774 \ud544\uc694\ud569\ub2c8\ub2e4. Stability \uc870\uac74 \\(\\Delta t &lt; 2 / \\lambda_\\mathrm{max}\\) \ub54c\ubb38\uc5d0 \ud070 learning rate\uc744 \ucde8\ud560 \uc218 \uc5c6\uc73c\ubbc0\ub85c, optimization \uc18d\ub3c4\uac00 \ud544\uc5f0\uc801\uc73c\ub85c \ub290\ub824\uc9c0\uac8c \ub429\ub2c8\ub2e4.</p> <p>PINN loss setup\uc73c\ub85c \ub3cc\uc544\uc624\uaca0\uc2b5\ub2c8\ub2e4.</p> <p>$$ L_\\mathrm{PINN}(\\theta) = L_\\mathrm{PDE}(\\theta) + \\lambda L_\\mathrm{BC}(\\theta) $$ \uc785\ub2c8\ub2e4. \ucef4\ud4e8\ud130\ub85c \\(H_\\mathrm{PDE}\\)\uc640 \\(H_\\mathrm{BC}\\)\uc758 eigenvalue distribution\uc744 \uacc4\uc0b0\ud574\ubcf4\uba74 PDE \ucabd spectrum\uc774 \ud6e8\uc52c \ub113\uc740 \uac83\uc744 \ubc1c\uacac\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub530\ub77c\uc11c PINN\uc758 optimization\uc744 \ubc29\ud574\ud558\ub294 \ubd80\ubd84\uc740 boundary condition \ubcf4\ub2e4\ub294 PDE loss \ucabd\uc774\ub77c\uace0 \uc774\ud574\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p> <ol> <li> <p>Wang, S., Teng, Y., &amp; Perdikaris, P. (2021) Understanding and mitigating gradient flow pathologies in physics-informed neural networks. SIAM Journal on Scientific Computing, 43, A3055\u2013A3081.\u00a0\u21a9</p> </li> </ol>"},{"location":"3.%20Training/NeuralTangentKernel/","title":"Neural Tangent Kernel Theory","text":""},{"location":"3.%20Training/NeuralTangentKernel/#neural-tangent-kernel-perspectives","title":"Neural Tangent Kernel Perspectives","text":"<p>Gradient flow\ub294 gradient descent\uc5d0\uc11c step size \\(\\eta\\)\ub97c \uc544\uc8fc \uc791\uac8c \ud588\uc744 \ub54c \uc5bb\uc744 \uc218 \uc788\uc5c8\uc2b5\ub2c8\ub2e4. \ud55c\ud3b8, neural network\uc758 width\ub97c \ud55c\uc5c6\uc774 \ud06c\uac8c \ub298\ub9ac\uac8c \ub418\uba74 Gaussian process\uac00 \ub41c\ub2e4\ub294 \uc5f0\uad6c\uacb0\uacfc\uac00 \uc788\uc2b5\ub2c8\ub2e4<sup>1</sup>. \uc774\ub97c \ud1b5\ud574\uc11c neural network\ub97c \uc774\ub860\uc801\uc73c\ub85c \ubd84\uc11d\ud558\ub294 \ubc29\ubc95 \uc911 \ud558\ub098\uac00 neural tangent kernel theory \uc785\ub2c8\ub2e4<sup>2</sup>. \uac19\uc740 analysis\ub97c PINN\uc5d0\ub3c4 \uc801\uc6a9\ud574 \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uadf8 \uacb0\uacfc\uac00 \uc5ec\uae30<sup>3</sup>\uc5d0 \uc815\ub9ac\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.</p> <ol> <li> <p>Lee, J., Bahri, Y., Novak, R., Schoenholz, S.S., Pennington, J., &amp; Sohl-Dickstein, J. (2017) Deep neural networks as gaussian processes. arXiv preprint arXiv:1711.00165.\u00a0\u21a9</p> </li> <li> <p>Jacot, A., Gabriel, F., &amp; Hongler, C. (2018) Neural tangent kernel: Convergence and generalization in neural networks. Advances in neural information processing systems, 31.\u00a0\u21a9</p> </li> <li> <p>Wang, S., Yu, X., &amp; Perdikaris, P. (2022) When and why PINNs fail to train: A neural tangent kernel perspective. Journal of Computational Physics, 449, 110768.\u00a0\u21a9</p> </li> </ol>"},{"location":"3.%20Training/main/","title":"Limitations of PINNs","text":"<p>\ubcf8 \ucc55\ud130\uc5d0\uc11c\ub294 \ud604\uc7ac PINN\uc774 \uac00\uc9c0\uace0 \uc788\ub294 \ub2e8\uc810\ub4e4\uc744 \uc124\uba85\ud569\ub2c8\ub2e4.</p> <p>PINN\uc740 \ud06c\uac8c \ub450 \uac00\uc9c0 \uce74\ud14c\uace0\ub9ac\uc758 \ubb38\uc81c\uc810\uc774 \uc788\uc2b5\ub2c8\ub2e4. 1. Training\uc774 \ub290\ub9bd\ub2c8\ub2e4. 2. Training\uc774 \uc2e4\ud328\ud558\ub294 \uacbd\uc6b0\uac00 \uaf64 \ub9ce\uc2b5\ub2c8\ub2e4.</p> <pre><code>PINN loss\ub97c minimization \ud558\ub294 \uac83\uc744 \ubcf4\ud1b5 optimization \uc774\ub77c\uace0 \ubd80\ub974\uace0, machine learning community\uc5d0\uc11c\ub294 training\uc774\ub77c\uace0 \ubd80\ub985\ub2c8\ub2e4.\n</code></pre>"},{"location":"3.%20Training/main/#slow-training","title":"Slow Training","text":"<p>\ubbf8\ubd84\ubc29\uc815\uc2dd\uc758 \ud574\ub97c \uc218\uce58\uc801\uc73c\ub85c \uad6c\ud55c\ub2e4\ub294 \uad00\uc810\uc5d0\uc11c \ubcf4\uba74, PINN\uc740 classical methods\ubcf4\ub2e4 \ud6e8\uc52c \ub290\ub9bd\ub2c8\ub2e4. Classical method\ub4e4\uc740 \ub300\ubd80\ubd84 \ubbf8\ubd84\ubc29\uc815\uc2dd\uc744 \uc120\ud615\ub300\uc218 \ubb38\uc81c\ub85c \ub9cc\ub4e4\uace0, \uc774\ubbf8 \uac1c\ubc1c\ub41c \ube60\ub974\uace0 \uc815\ud655\ud55c \uc54c\uace0\ub9ac\uc998\ub4e4\uc744 \uac00\uc838\ub2e4 \uc0ac\uc6a9\ud558\uba74 \ub418\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4. \ubc18\uba74 PINN\uc740 \ubbf8\ubd84\ubc29\uc815\uc2dd\uc744 optimization \ubb38\uc81c\ub85c \ub9cc\ub4ed\ub2c8\ub2e4. Convexity, linearity \ub458 \ub2e4 \uc5c6\uae30 \ub54c\ubb38\uc5d0 \ubcf4\ud1b5 \ud574\uacb0\ud558\uae30 \ubcf5\uc7a1\ud55c \ubb38\uc81c\uc785\ub2c8\ub2e4.</p> <p>\uc608\ub97c \ub4e4\uc5b4, \uc774\uc804 \ucc55\ud130\uc5d0\uc11c \uacf5\ubd80\ud55c 1d Poisson equation\uc758 \uacbd\uc6b0 PINN\uc740 GPU\uc5d0\uc11c 1\ubd84 \ub0a8\uc9d3\uc774 \uac78\ub838\uc9c0\ub9cc, Central difference finite difference method\ub294 1\ucd08\ub3c4 \uac78\ub9ac\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4. \ub530\ub77c\uc11c \ud604\uc7ac PDE\ub97c \ud478\ub294\ub370 PINNs\ub97c \uc774\uc6a9\ud558\ub294 \uac83\uc740 \uadf8\ub9ac \ubc14\ub78c\uc9c1\ud558\ub2e4\uace0 \ubcfc \uc218 \uc5c6\uc2b5\ub2c8\ub2e4. Super high dimension\uc73c\ub85c \uac00\uac8c \ub418\uba74 \ub610 \ud560 \ub9d0\uc774 \uc0dd\uae38 \uc218 \uc788\uaca0\uc2b5\ub2c8\ub2e4\ub9cc, \uc801\uc5b4\ub3c4 (6 + 1) \ucc28\uc6d0 (e.g. Boltzmann equation) \uae4c\uc9c0\ub294 polynomial-based \ubc29\ubc95\ubcf4\ub2e4 \ub290\ub9bd\ub2c8\ub2e4<sup>1</sup>.</p> <p>\uadf8\ub7fc\uc5d0\ub3c4 \ubd88\uad6c\ud558\uace0 \ubcf8 \ucc45\uc740 PINN\uc744 \ud1b5\ud574 \ubbf8\ubd84\ubc29\uc815\uc2dd\uc744 \ud478\ub294 \ubc29\ubc95\uc5d0 \ub300\ud574 \uc124\uba85\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uc815\ud655\ud788\ub294 PINN loss\ub97c minimization \ud558\ub294 \ubc29\ubc95\uc5d0 \ub300\ud574 \uc124\uba85\ud558\uace0 \uc788\ub2e4\uace0 \ubcf4\uc544\uc57c \ud569\ub2c8\ub2e4. \ubd80\uc801\uc808\ud55c minimization \ubc29\ubc95\uc744 \uc0ac\uc6a9\ud574 \ubbf8\ubd84\ubc29\uc815\uc2dd \uc815\ubcf4\ub97c machine learning \ubaa8\ub378\uc5d0 \uc8fc\uc785\ud558\uac8c \ub41c\ub2e4\uba74 \ud37c\ud3ec\uba3c\uc2a4\uc5d0 \ub3c4\uc6c0\uc740 \ucee4\ub155 \ubd80\uc815\uc801\uc778 \uc601\ud5a5\uc744 \ub07c\uce60 \uc218 \uc788\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4. \ub610\ud55c PINN loss\uc5d0 \ub300\ud574\uc11c \uc9c0\uc2dd\uc774 \uc313\uc774\ub2e4 \ubcf4\uba74 classical numerical method\ubcf4\ub2e4 \uc88b\uc740 \ud37c\ud3ec\uba3c\uc2a4\ub97c \ub0bc \uc218 \uc788\ub294 \uc54c\uace0\ub9ac\uc998\uc774 \ub098\uc62c\uc9c0\ub3c4 \ubaa8\ub974\ub294 \uc77c\uc785\ub2c8\ub2e4.</p>"},{"location":"3.%20Training/main/#training-failures","title":"Training Failures","text":"<p>Optimization\uc774 \uc798 \ub418\uc5b4\uc11c (ideal) PINN loss {prf:def}<code>ideal-PINN-Loss</code>\uac00 \uc815\ud655\ud558\uac8c \\(0\\)\uc774 \ub41c\ub2e4\uba74 \uc544\ubb34\ub7f0 \ubb38\uc81c\uac00 \uc5c6\uc2b5\ub2c8\ub2e4. \ud558\uc9c0\ub9cc symbolic \uacc4\uc0b0\uc744 \ud558\ub294 \uac83\uc774 \uc544\ub2c8\ub77c\uba74, \uc5b4\ub824\uac00\uc9c0 \uc774\uc720 \ub54c\ubb38\uc5d0 \uc815\ud655\ud558\uac8c \\(0\\)\uc744 \ub2ec\uc131\ud560 \uc218\ub294 \uc5c6\uc2b5\ub2c8\ub2e4. \uc2ec\uc9c0\uc5b4, loss\uac00 \uc798 \uc904\uc5b4\ub4e0 \uac83 \ucc98\ub7fc \ubcf4\uc774\uc9c0\ub9cc \ubbf8\ubd84\ubc29\uc815\uc2dd\uc758 \ud574\ub97c \uc798 approximation\ud558\uc9c0 \ubabb\ud558\ub294 \uacbd\uc6b0\ub3c4 \uc788\uc2b5\ub2c8\ub2e4<sup>2</sup>. \ubcf8 \ucc55\ud130\uc5d0\uc11c\ub294 \uc774 \uacbd\uc6b0\ub97c \uc911\uc810\uc801\uc73c\ub85c \uc124\uba85\ud569\ub2c8\ub2e4.</p> <p>PINN\uc740 \uacb0\uad6d neural network\ub97c \ud1b5\ud574 PINN Loss {eq}<code>PINN-Loss</code>\ub97c \uc904\uc5ec\uc11c neural network\uc5d0 \ubbf8\ubd84\ubc29\uc815\uc2dd \uc815\ubcf4\ub97c \uc8fc\uc785\ud558\ub294 \ubc29\ubc95\uc785\ub2c8\ub2e4. \ud558\uc9c0\ub9cc \uc2e4\uc81c\ub85c PINN\uc744 \ud1b5\ud574 \ubbf8\ubd84\ubc29\uc815\uc2dd\uc744 \ud480\uc5b4\ubcf4\ub824\uace0 \uc2dc\ub3c4\ud558\uba74, \uc0dd\uac01\ubcf4\ub2e4 \uc798 \ub418\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4. \uc5ec\uae30\uc5d0\ub294 \ud06c\uac8c \ub450\uac00\uc9c0 \uc774\uc720\uac00 \uc788\uc2b5\ub2c8\ub2e4. 1. model\uc758 expressibility \ubd80\uc871 \ubb38\uc81c 2. Optimization \ubb38\uc81c</p>"},{"location":"3.%20Training/main/#expressibility-issues","title":"Expressibility Issues","text":"<p>\ub9cc\uc57d \ubbf8\ubd84\ubc29\uc815\uc2dd\uc758 \ud574\uac00 \uc5c4\uccad \ubcf5\uc7a1\ud55c \ud568\uc218\ub77c\uba74, \ubcf4\ud1b5 model\uc758 parameter \uac1c\uc218\ub97c \ucda9\ubd84\ud788 \ub9ce\ub3c4\ub85d \ud574 \uc8fc\uc5b4\uc57c \ud569\ub2c8\ub2e4. \ucda9\ubd84\ud558\uc9c0 \uc54a\ub2e4\uba74, {prf:ref}<code>universal-approximation-theorem</code>\uc744 \ub9cc\uc871\ud558\uc9c0 \ubabb\ud558\uace0, model\uc774 \ud574\ub97c \uc798 approximation \ud560 \uc218 \uc788\ub2e4\ub294 \uc774\ub860\uc801 \ubcf4\uc7a5\uc774 \uae68\uc9c0\uac8c \ub429\ub2c8\ub2e4.</p>"},{"location":"3.%20Training/main/#optimization-issues","title":"Optimization Issues","text":"<p>PINN loss\uc758 minimum\uc744 \ucc3e\ub294 \uc77c\uc740 \uac04\ub2e8\ud558\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4. Neural network\ub97c model\ub85c \uc0ac\uc6a9\ud560 \uacbd\uc6b0\uc5d0 PINN loss\ub294 non-convex objective function\uc785\ub2c8\ub2e4. \uc774 \uacbd\uc6b0 gradient descent\uac00 global minimum\uc73c\ub85c \uc218\ub834\ud55c\ub2e4\ub294 \uc774\ub860\uc801\uc778 \ubcf4\uc7a5\uc740 \uc5c6\uc2b5\ub2c8\ub2e4.</p> <ol> <li> <p>Oh, J., Cho, S.Y., Yun, S.-B., Park, E., &amp; Hong, Y. (2024) Separable physics-informed neural networks for solving the BGK model of the boltzmann equation. arXiv preprint arXiv:2403.06342.\u00a0\u21a9</p> </li> <li> <p>Krishnapriyan, A., Gholami, A., Zhe, S., Kirby, R., &amp; Mahoney, M.W. (2021) Characterizing possible failure modes in physics-informed neural networks. Advances in neural information processing systems, vol. 34 (Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, P.S., &amp; Vaughan, J.W. eds). Curran Associates, Inc., pp. 26548\u201326560.\u00a0\u21a9</p> </li> </ol>"},{"location":"5.%20Etc/Advices/","title":"Practical Advices","text":""},{"location":"5.%20Etc/Advices/#how-to-choose-width-and-depth","title":"How to choose width and depth","text":"<p>\uc778\uacf5\uc2e0\uacbd\ub9dd\uc758 width, depth\ub97c \uc815\ud558\ub294 \uc77c\ubc18\uc801\uc778 \ubc29\ubc95\uc740 \uc798 \uc54c\ub824\uc838 \uc788\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4. \ub9cc\uc57d numerical solution\uc774\ub098 analytic solution\uc774 \uc788\ub2e4\uba74, \uc774\ub97c \uc778\uacf5\uc2e0\uacbd\ub9dd\uc73c\ub85c regression\ud574\uc11c \uc9c0\uae08 \uc0ac\uc6a9\ud558\uace0 \uc788\ub294 width, depth\uac00 \uc801\ub2f9\ud55c\uc9c0 \ud655\uc778\ud574\ubcf4\ub294\uac8c \uc88b\uc2b5\ub2c8\ub2e4. Regression\uc774 \uc548\ub418\ub294\ub370 PINN optimization\uc774 \ub418\ub294 \uacbd\uc6b0\ub294 \ud754\uce58 \uc54a\uc2b5\ub2c8\ub2e4.</p> <p>\uc774\ubbf8 numerical solution\uc774\ub098 analytic solution\uc774 \uc788\ub2e4\uba74, \uadf8\uac83\uc744 \uc0ac\uc6a9\ud574\uc11c \ubbf8\ub9ac width\uc640 depth\ub97c \uc815\ud558\ub294 \uac83\uc740 \ubc18\uce59 \uc544\ub2c8\ub0d0\uace0 \uc774\uc57c\uae30 \ud560 \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub294 \ub2e8\uc9c0 \uc77c\uc744 \ud6a8\uc728\uc801\uc73c\ub85c \ud558\uae30 \uc704\ud55c \uc870\uc5b8\uc785\ub2c8\ub2e4. \uc774\ub807\uac8c regression\uc744 \ud1b5\ud574 \ucc3e\uc740 width\uc640 depth\ub97c \uae30\uc900\uc73c\ub85c \ube60\ub974\uac8c prototyping\ud574\uc11c algorithm\uc758 \ud37c\ud3ec\uba3c\uc2a4\ub97c \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub9cc\uc57d \ub17c\ubb38\uc744 \uc791\uc131\ud558\uc2e0\ub2e4\uba74, \"numerical solution\uc744 regression\ud574\uc11c width\uc640 depth\ub97c \ucc3e\uc558\ub2e4\"\ub77c\uace0 \uc801\uc73c\uba74 \ub2f9\uc5f0\ud788 \uc548\ub418\uace0, width\uc640 depth\ub97c \uc870\uc808\ud574\uac00\uba74\uc11c \uc5d0\ub7ec\uac00 \uc5b4\ub5bb\uac8c \ubcc0\ud558\ub294\uc9c0 \uc870\uc0ac\ud55c table\uc744 \ucca8\ubd80\ud574\uc8fc\uba74 \uc88b\uc2b5\ub2c8\ub2e4.</p>"},{"location":"5.%20Etc/SPINNs/","title":"Separable Physics-informed Neural Networks","text":"<p>\ud568\uc218 \\(f: \\mathbb{R}^d \\rightarrow \\mathbb{R}\\)\uc774 \uc788\uc744 \ub54c, \uc9c0\uae08\uae4c\uc9c0\ub294 \\begin{equation}     f(x_1, \\dots, x_d) \\approx \\mathrm{MLP}(x_1, \\dots, x_d ; \\theta) \\end{equation} \ub85c \ud568\uc218\ub97c approximation \ud574 \uc654\uc5c8\uc2b5\ub2c8\ub2e4.</p> <p>\uc774 \ud615\ud0dc \ub300\uc2e0 \\begin{equation}     f(x_1, \\dots, x_d) \\approx \\sum_{r=1}^R \\otimes_{i=1}^d \\mathrm{MLP}(x_i; \\theta_i) \\end{equation} \uc758 \ud615\ud0dc\ub85c PDE\uc758 solution\uc744 approximation \ud558\ub294 \ubc29\ubc95\uc744 separable physics-informed neural networks \ub77c\uace0 \ubd80\ub985\ub2c8\ub2e4<sup>1</sup>.</p> <p>\uadf8\ub0e5 \ubd10\uc11c\ub294 \uc5b4\ub5bb\uac8c speed-up\uc774 \uc788\ub294\uc9c0 \uac10\uc774 \uc624\uc9c0 \uc54a\uc744 \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4. \ud558\uc9c0\ub9cc \uc800 \ud615\ud0dc\ub294 rectilinear grid \uc704\uc5d0\uc11c vectorize \ud558\uc5ec \uacc4\uc0b0\ud558\uac8c \ub418\uba74, forward pass\uc758 \ud69f\uc218\uac00 \\(O(N^d)\\)\uc5d0\uc11c \\(O(dN)\\)\uc73c\ub85c \uc904\uc5b4\ub4e4\uac8c \ub429\ub2c8\ub2e4. \ub192\uc740 dimension\uc758 rectilinear grid\uc5d0\uc11c \ub9e4\uc6b0 \ube60\ub978 \uacc4\uc0b0 \uc18d\ub3c4\ub97c \ubcf4\uc5ec\uc8fc\uac8c \ub429\ub2c8\ub2e4. \uc774\ub294 singular value decomposition, \ud639\uc740 canonical polyadic decomposition\uc73c\ub85c\ub3c4 \uc798 \uc54c\ub824\uc838 \uc788\ub294 \ubc29\ubc95\uc785\ub2c8\ub2e4.</p> <pre><code>:label: remark-TensorNeuralNetworks\nTensor neural networks (TNNs)\ub77c\ub294 \uc774\ub984\uc73c\ub85c\ub3c4 \uc54c\ub824\uc838 \uc788\uc73c\ub098[^2], arXiv\uc5d0 \ub4f1\uc7a5\ud55c \ub0a0\uc9dc \uae30\uc900\uc73c\ub85c SPINNs\uac00 \uba87\uac1c\uc6d4 \uc815\ub3c4 \ube60\ub985\ub2c8\ub2e4.\n\ub610\ud55c TNN\uc740 CANDECOMP\uc758 \ud615\ud0dc\ub9cc \uc0ac\uc6a9\ud568\uc5d0\ub3c4 \ubd88\uad6c\ud558\uace0 \"tensor\"\ub77c\ub294 \uc77c\ubc18\uc801\uc778 \uc774\ub984\uc744 \ubd99\uc600\uc2b5\ub2c8\ub2e4.\nTensor method\uc5d0\ub294 \uc5ec\ub7ec\uac00\uc9c0 \ubc29\ubc95\uc774 \uc788\uc2b5\ub2c8\ub2e4. CANDECOMP, Tucker format, tensor train format \ub4f1.\n</code></pre> <ol> <li> <p>Cho, J., Nam, S., Yang, H., Yun, S.-B., Hong, Y., &amp; Park, E. (2024) Separable physics-informed neural networks. Advances in Neural Information Processing Systems, 36.\u00a0\u21a9</p> </li> <li> <p>Wang, Y., Jin, P., &amp; Xie, H. (2022) Tensor neural network and its numerical integration. arXiv preprint arXiv:2207.02754.\u00a0\u21a9</p> </li> </ol>"},{"location":"5.%20Etc/TaylorAD/","title":"Taylor Mode Automatic Differentiation","text":"<p>\ubbf8\ubd84\uc744 \ub354 \ub9ce\uc774 \ud560 \uc218\ub85d \uacc4\uc0b0 \ube44\uc6a9\uc774 \uc99d\uac00\ud569\ub2c8\ub2e4. \uba3c\uc800 finite difference formula\ub97c \uc608\ub85c \ub4e4\uc5b4\ubcf4\uaca0\uc2b5\ub2c8\ub2e4. \ud568\uc218 \\(f: \\mathbb{R} \\rightarrow \\mathbb{R}\\)\uc774 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ub54c 1\uacc4 \ubbf8\ubd84\uc740 \\begin{equation}     f'(x) \\approx \\frac{f(x+h) - f(x-h)}{2h} \\end{equation} \ub85c \ud568\uc218 \\(f\\)\ub97c \ub450\ubc88 evaluate\ud558\uc5ec \ubbf8\ubd84\uc744 approximation \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ud558\uc9c0\ub9cc \uc774\uacc4 \ubbf8\ubd84\uc740 \\begin{equation}     f''(x) \\approx \\frac{f(x+h) - 2f(x) + f(x-h)}{h^2} \\end{equation} \ud568\uc218 \\(f\\)\ub97c \uc138\ubc88 evaluate \ud574\uc57c \ud569\ub2c8\ub2e4. \ub530\ub77c\uc11c \ubbf8\ubd84\uc744 \ub354 \ub9ce\uc774 \ud560 \uc218\ub85d \ud568\uc218 \\(f\\)\ub97c \ub354 \ub9ce\uc774 \uacc4\uc0b0\ud574\uc57c\ud558\ubbc0\ub85c, \uacc4\uc0b0 \ube44\uc6a9\uc774 \uc99d\uac00\ud55c\ub2e4\uace0 \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>Automatic differentiation\uc758 \uacbd\uc6b0\ub3c4 \ub9c8\ucc2c\uac00\uc9c0\uc785\ub2c8\ub2e4. Forward mode AD\ub85c \\(f'(x)\\)\ub97c \uacc4\uc0b0\ud558\ub294 \uac83\uc740 \uc57d \\(f\\) \ube44\uc6a9\uc758 \uc138\ubc30\uc785\ub2c8\ub2e4. \ubbf8\ubd84\uc744 \ud55c\ubc88 \ub354 \ud574\uc11c \\(f''(x)\\)\ub97c \uacc4\uc0b0\ud558\uac8c \ub418\uba74, \uc774\ub294 \\(f'(x)\\)\ub97c \uacc4\uc0b0\ud558\ub294 \ube44\uc6a9\uc758 \uc138\ubc30\uac00 \ub418\ubbc0\ub85c \\(f(x)\\)\ub97c \uacc4\uc0b0\ud558\ub294 \ube44\uc6a9\uc758 9\ubc30\uac00 \ub429\ub2c8\ub2e4. Finite difference formula\uc640\ub294 \ub2e4\ub974\uac8c forward mode AD\uc758 \uacc4\uc0b0 \ube44\uc6a9\uc740 \ubbf8\ubd84\uc758 \ud69f\uc218\uac00 \uc99d\uac00\ud558\uac8c \ub418\uba74 \uc9c0\uc218\uc801\uc73c\ub85c \uc99d\uac00\ud569\ub2c8\ub2e4.</p> <p>High-order \ubbf8\ubd84\uc774 \uc788\ub294 \ubc29\uc815\uc2dd\uc758 \uc608\uc2dc\ub97c \ud558\ub098 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4. <pre><code>:label: example-kuramoto-sivashinsky\n\nKuramoto-Sivashinsky \ubc29\uc815\uc2dd\uc740 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4.\n\\begin{equation*}\n    \\partial_t u + 0.5 \\partial_x (u^2) + \\partial_x^2 u + \\partial_x^4 u = 0.\n\\end{equation*}\n</code></pre> \\(x\\)\uc5d0 \ub300\ud55c \ubbf8\ubd84\uc744 4\ubc88\uae4c\uc9c0 \ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4. Forward mode AD\ub85c na\u00efve\ud558\uac8c \uacc4\uc0b0\ud558\uba74 \\(u\\) \uacc4\uc0b0 \ube44\uc6a9\uc758 \\(3^4 = 81\\)\ubc30\uac00 \ud544\uc694\ud569\ub2c8\ub2e4.</p> <p>\uc774 \ub54c Taylor mode AD\ub97c \uc774\uc6a9\ud558\uc5ec high order differential\uc744 \ube60\ub974\uac8c \uacc4\uc0b0\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p>"},{"location":"5.%20Etc/main/","title":"Speed up techniques","text":""}]}